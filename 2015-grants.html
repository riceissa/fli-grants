<!DOCTYPE html>
<html lang="en-US" prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#" class="html_stretched responsive av-preloader-disabled av-default-lightbox  html_header_top html_logo_left html_bottom_nav_header html_menu_left html_large html_header_sticky html_header_shrinking html_header_topbar_active html_mobile_menu_phone html_disabled html_header_searchicon html_content_align_center html_header_unstick_top_disabled html_header_stretch_disabled html_minimal_header html_entry_id_875 ">
<head>
<meta charset="UTF-8" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<title>First AI Grant Recipients - Future of Life Institute</title>

<link rel="canonical" href="https://futureoflife.org/first-ai-grant-recipients/" />
<meta property="og:locale" content="en_US" />
<meta property="og:type" content="article" />
<meta property="og:title" content="First AI Grant Recipients - Future of Life Institute" />
<meta property="og:url" content="https://futureoflife.org/first-ai-grant-recipients/" />
<meta property="og:site_name" content="Future of Life Institute" />
<meta property="article:publisher" content="https://www.facebook.com/futureoflifeinstitute" />
<meta property="fb:app_id" content="852322561461934" />
<meta property="og:image" content="http://futureoflife.org/wp-content/uploads/2016/04/FLI_logo_square_250.png" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="First AI Grant Recipients - Future of Life Institute" />
<meta name="twitter:site" content="@FLIxrisk" />
<meta name="twitter:image" content="http://futureoflife.org/wp-content/uploads/2016/04/FLI_logo_square_250.png" />
<meta name="twitter:creator" content="@FLIxrisk" />
<script type='application/ld+json'>{"@context":"http:\/\/schema.org","@type":"WebSite","@id":"#website","url":"https:\/\/futureoflife.org\/","name":"Future of Life Institute","potentialAction":{"@type":"SearchAction","target":"https:\/\/futureoflife.org\/?s={search_term_string}","query-input":"required name=search_term_string"}}</script>

<link rel='dns-prefetch' href='//api.tiles.mapbox.com' />
<link rel='dns-prefetch' href='//s.w.org' />
<link rel="alternate" type="application/rss+xml" title="Future of Life Institute &raquo; Feed" href="https://futureoflife.org/feed/" />
<link rel="alternate" type="application/rss+xml" title="Future of Life Institute &raquo; Comments Feed" href="https://futureoflife.org/comments/feed/" />
<link rel="alternate" type="text/calendar" title="Future of Life Institute &raquo; iCal Feed" href="https://futureoflife.org/events/?ical=1" />

<link rel='stylesheet' id='avia-google-webfont' href='//fonts.googleapis.com/css?family=Open+Sans:400,600' type='text/css' media='all' />
<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2.3\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2.3\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/futureoflife.org\/wp-includes\/js\/wp-emoji-release.min.js?ver=7c224e936f87abf4fda6ad48adcc9398"}};
			!function(a,b,c){function d(a){var b,c,d,e,f=String.fromCharCode;if(!k||!k.fillText)return!1;switch(k.clearRect(0,0,j.width,j.height),k.textBaseline="top",k.font="600 32px Arial",a){case"flag":return k.fillText(f(55356,56826,55356,56819),0,0),b=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,56826,8203,55356,56819),0,0),c=j.toDataURL(),b!==c&&(k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447),0,0),b=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447),0,0),c=j.toDataURL(),b!==c);case"emoji4":return k.fillText(f(55358,56794,8205,9794,65039),0,0),d=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55358,56794,8203,9794,65039),0,0),e=j.toDataURL(),d!==e}return!1}function e(a){var c=b.createElement("script");c.src=a,c.defer=c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var f,g,h,i,j=b.createElement("canvas"),k=j.getContext&&j.getContext("2d");for(i=Array("flag","emoji4"),c.supports={everything:!0,everythingExceptFlag:!0},h=0;h<i.length;h++)c.supports[i[h]]=d(i[h]),c.supports.everything=c.supports.everything&&c.supports[i[h]],"flag"!==i[h]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[i[h]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(g=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",g,!1),a.addEventListener("load",g,!1)):(a.attachEvent("onload",g),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),f=c.source||{},f.concatemoji?e(f.concatemoji):f.wpemoji&&f.twemoji&&(e(f.twemoji),e(f.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
<link rel='stylesheet' id='mapboxadvcss-css' href='https://futureoflife.org/wp-content/plugins/mapbox-for-wp-advanced/assets/css/mapboxadv-min.css?x57718' type='text/css' media='all' />
<link rel='stylesheet' id='mapboxcss-css' href='https://api.tiles.mapbox.com/mapbox.js/v2.2.2/mapbox.css?ver=7c224e936f87abf4fda6ad48adcc9398' type='text/css' media='all' />
<link rel='stylesheet' id='h5p-plugin-styles-css' href='https://futureoflife.org/wp-content/plugins/h5p/h5p-php-library/styles/h5p.css?x57718' type='text/css' media='all' />
<link rel='stylesheet' id='avia-grid-css' href='https://futureoflife.org/wp-content/themes/enfold-3/css/grid.css?x57718' type='text/css' media='all' />
<link rel='stylesheet' id='avia-base-css' href='https://futureoflife.org/wp-content/themes/enfold-3/css/base.css?x57718' type='text/css' media='all' />
<link rel='stylesheet' id='avia-layout-css' href='https://futureoflife.org/wp-content/themes/enfold-3/css/layout.css?x57718' type='text/css' media='all' />
<link rel='stylesheet' id='avia-scs-css' href='https://futureoflife.org/wp-content/themes/enfold-3/css/shortcodes.css?x57718' type='text/css' media='all' />
<link rel='stylesheet' id='avia-popup-css-css' href='https://futureoflife.org/wp-content/themes/enfold-3/js/aviapopup/magnific-popup.css?x57718' type='text/css' media='screen' />
<link rel='stylesheet' id='avia-media-css' href='https://futureoflife.org/wp-content/themes/enfold-3/js/mediaelement/skin-1/mediaelementplayer.css?x57718' type='text/css' media='screen' />
<link rel='stylesheet' id='avia-print-css' href='https://futureoflife.org/wp-content/themes/enfold-3/css/print.css?x57718' type='text/css' media='print' />
<link rel='stylesheet' id='avia-dynamic-css' href='https://futureoflife.org/wp-content/uploads/dynamic_avia/enfold.css?x57718' type='text/css' media='all' />
<link rel='stylesheet' id='avia-custom-css' href='https://futureoflife.org/wp-content/themes/enfold-3/css/custom.css?x57718' type='text/css' media='all' />
<link rel='stylesheet' id='mc4wp-form-basic-css' href='https://futureoflife.org/wp-content/plugins/mailchimp-for-wp/assets/css/form-basic.min.css?x57718' type='text/css' media='all' />
<link rel='stylesheet' id='avia-events-cal-css' href='https://futureoflife.org/wp-content/themes/enfold-3/config-events-calendar/event-mod.css?x57718' type='text/css' media='all' />
<script type='text/javascript' src='https://api.tiles.mapbox.com/mapbox.js/v2.2.2/mapbox.js?ver=1.3.2'></script>
<script type='text/javascript' src='https://futureoflife.org/wp-includes/js/jquery/jquery.js?x57718'></script>
<script type='text/javascript' src='https://futureoflife.org/wp-includes/js/jquery/jquery-migrate.min.js?x57718'></script>
<script type='text/javascript' src='https://futureoflife.org/wp-content/themes/enfold-3/js/avia-compat.js?x57718'></script>
<link rel='https://api.w.org/' href='https://futureoflife.org/wp-json/' />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://futureoflife.org/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://futureoflife.org/wp-includes/wlwmanifest.xml" />
<link rel='shortlink' href='https://futureoflife.org/?p=875' />
<link rel="alternate" type="application/json+oembed" href="https://futureoflife.org/wp-json/oembed/1.0/embed?url=https%3A%2F%2Ffutureoflife.org%2Ffirst-ai-grant-recipients%2F" />
<link rel="alternate" type="text/xml+oembed" href="https://futureoflife.org/wp-json/oembed/1.0/embed?url=https%3A%2F%2Ffutureoflife.org%2Ffirst-ai-grant-recipients%2F&#038;format=xml" />
<script type="text/javascript">
(function(url){
	if(/(?:Chrome\/26\.0\.1410\.63 Safari\/537\.31|WordfenceTestMonBot)/.test(navigator.userAgent)){ return; }
	var addEvent = function(evt, handler) {
		if (window.addEventListener) {
			document.addEventListener(evt, handler, false);
		} else if (window.attachEvent) {
			document.attachEvent('on' + evt, handler);
		}
	};
	var removeEvent = function(evt, handler) {
		if (window.removeEventListener) {
			document.removeEventListener(evt, handler, false);
		} else if (window.detachEvent) {
			document.detachEvent('on' + evt, handler);
		}
	};
	var evts = 'contextmenu dblclick drag dragend dragenter dragleave dragover dragstart drop keydown keypress keyup mousedown mousemove mouseout mouseover mouseup mousewheel scroll'.split(' ');
	var logHuman = function() {
		var wfscr = document.createElement('script');
		wfscr.type = 'text/javascript';
		wfscr.async = true;
		wfscr.src = url + '&r=' + Math.random();
		(document.getElementsByTagName('head')[0]||document.getElementsByTagName('body')[0]).appendChild(wfscr);
		for (var i = 0; i < evts.length; i++) {
			removeEvent(evts[i], logHuman);
		}
	};
	for (var i = 0; i < evts.length; i++) {
		addEvent(evts[i], logHuman);
	}
})('//futureoflife.org/?wordfence_lh=1&hid=E8ADECC1420B401850B1AFD75F39E524');
</script><meta name="tec-api-version" content="v1"><meta name="tec-api-origin" content="https://futureoflife.org"><link rel="https://theeventscalendar.com/" href="https://futureoflife.org/wp-json/tribe/events/v1/" /><link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="alternate" type="application/rss+xml" title="Future of Life Institute RSS2 Feed" href="https://futureoflife.org/feed/" />
<link rel="pingback" href="https://futureoflife.org/xmlrpc.php" />
<!--[if lt IE 9]><script src="https://futureoflife.org/wp-content/themes/enfold-3/js/html5shiv.js?x57718"></script><![endif]-->
<style type="text/css">/* MailChimp for WP - Checkbox Styles */
.mc4wp-checkbox-wp-registration-form {
  clear: both;
  display: block;
  position: static;
  width: auto; }
  .mc4wp-checkbox-wp-registration-form input {
    float: none;
    width: auto;
    position: static;
    margin: 0 6px 0 0;
    padding: 0;
    vertical-align: middle;
    display: inline-block !important;
    max-width: 21px;
    -webkit-appearance: checkbox; }
  .mc4wp-checkbox-wp-registration-form label {
    float: none;
    display: block;
    cursor: pointer;
    width: auto;
    position: static;
    margin: 0 0 16px 0; }
</style>
<style type="text/css"></style>
<style type="text/css">
.synved-social-resolution-single {
display: inline-block;
}
.synved-social-resolution-normal {
display: inline-block;
}
.synved-social-resolution-hidef {
display: none;
}

@media only screen and (min--moz-device-pixel-ratio: 2),
only screen and (-o-min-device-pixel-ratio: 2/1),
only screen and (-webkit-min-device-pixel-ratio: 2),
only screen and (min-device-pixel-ratio: 2),
only screen and (min-resolution: 2dppx),
only screen and (min-resolution: 192dpi) {
	.synved-social-resolution-normal {
	display: none;
	}
	.synved-social-resolution-hidef {
	display: inline-block;
	}
}
</style>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-60471514-1', 'auto');
  ga('send', 'pageview');
</script>

<style type="text/css" id="wp-custom-css">
			/*
You can add your own CSS here.

Click the help icon above to learn more.
*/

.page-id-11350 ul{
margin-bottom: 0px;
}

.page-id-11350 li{
margin-bottom: 0px;
}

.page-id-11350 .entry-content-wrapper li{
padding: 0;
}

.page-id-11350 p{
padding: 0;
margin: 0;
}

body .post-entry-11515 .column-top-margin{
margin-top: 0px;
}
		</style>

<style type='text/css'>
@font-face {font-family: 'entypo-fontello'; font-weight: normal; font-style: normal;
src: url('https://futureoflife.org/wp-content/themes/enfold-3/config-templatebuilder/avia-template-builder/assets/fonts/entypo-fontello.eot?v=3');
src: url('https://futureoflife.org/wp-content/themes/enfold-3/config-templatebuilder/avia-template-builder/assets/fonts/entypo-fontello.eot?v=3#iefix') format('embedded-opentype'), 
url('https://futureoflife.org/wp-content/themes/enfold-3/config-templatebuilder/avia-template-builder/assets/fonts/entypo-fontello.woff?v=3') format('woff'), 
url('https://futureoflife.org/wp-content/themes/enfold-3/config-templatebuilder/avia-template-builder/assets/fonts/entypo-fontello.ttf?v=3') format('truetype'), 
url('https://futureoflife.org/wp-content/themes/enfold-3/config-templatebuilder/avia-template-builder/assets/fonts/entypo-fontello.svg?v=3#entypo-fontello') format('svg');
} #top .avia-font-entypo-fontello, body .avia-font-entypo-fontello, html body [data-av_iconfont='entypo-fontello']:before{ font-family: 'entypo-fontello'; }
</style>
</head>
<body id="top" class="page-template-default page page-id-875 stretched open_sans tribe-no-js" itemscope="itemscope" itemtype="https://schema.org/WebPage">
<div id='wrap_all'>
<header id='header' class=' header_color dark_bg_color  av_header_top av_logo_left av_bottom_nav_header av_menu_left av_large av_header_sticky av_header_shrinking av_header_stretch_disabled av_mobile_menu_phone av_header_searchicon av_header_unstick_top_disabled av_minimal_header av_header_border_disabled' role="banner" itemscope="itemscope" itemtype="https://schema.org/WPHeader">
<a id="advanced_menu_toggle" href="#" aria-hidden='true' data-av_icon='' data-av_iconfont='entypo-fontello'></a><a id="advanced_menu_hide" href="#" aria-hidden='true' data-av_icon='' data-av_iconfont='entypo-fontello'></a> <div id='header_meta' class='container_wrap container_wrap_meta  av_icon_active_right av_extra_header_active av_secondary_left av_entry_id_875'>
<div class='container'>
<ul class='noLightbox social_bookmarks icon_count_2'><li class='social_bookmarks_twitter av-social-link-twitter social_icon_1'><a target='_blank' href='http://twitter.com/FLIxrisk' aria-hidden='true' data-av_icon='' data-av_iconfont='entypo-fontello' title='Twitter'><span class='avia_hidden_link_text'>Twitter</span></a></li><li class='social_bookmarks_facebook av-social-link-facebook social_icon_2'><a target='_blank' href='https://www.facebook.com/futureoflifeinstitute' aria-hidden='true' data-av_icon='' data-av_iconfont='entypo-fontello' title='Facebook'><span class='avia_hidden_link_text'>Facebook</span></a></li></ul><nav class='sub_menu' role="navigation" itemscope="itemscope" itemtype="https://schema.org/SiteNavigationElement"><ul id="avia2-menu" class="menu"><li id="menu-item-4" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-4"><a href="http://futureoflife.org/">Home</a></li>
<li id="menu-item-93" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-93"><a href="https://futureoflife.org/team/">Who We Are</a>
<ul class="sub-menu">
<li id="menu-item-10891" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-10891"><a href="https://futureoflife.org/team/">Team</a></li>
<li id="menu-item-2423" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-2423"><a href="http://futureoflife.org/wp-content/uploads/2016/02/FLI-2015-Annual-Report.pdf?x57718">2015 Annual Report</a></li>
<li id="menu-item-13129" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-13129"><a href="https://futureoflife.org/wp-content/uploads/2017/05/FLI-2016-Annual-Report.pdf?x57718">2016 Annual Report</a></li>
<li id="menu-item-10994" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-10994"><a href="https://futureoflife.org/tax-forms/">Tax Forms</a></li>
</ul>
</li>
<li id="menu-item-819" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-819"><a href="https://futureoflife.org/activities-2/">Activities</a>
<ul class="sub-menu">
<li id="menu-item-815" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-815"><a href="https://futureoflife.org/ai-activities/">AI</a></li>
<li id="menu-item-3563" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-3563"><a href="http://futureoflife.org/events/">Upcoming Events</a></li>
<li id="menu-item-2026" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2026"><a href="https://futureoflife.org/past_events/">Past Events</a></li>
<li id="menu-item-84" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-84"><a href="https://futureoflife.org/press/">Press</a></li>
<li id="menu-item-2029" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2029"><a href="https://futureoflife.org/newsletters/">Newsletters</a></li>
</ul>
</li>
<li id="menu-item-1427" class="menu-item menu-item-type-post_type menu-item-object-portfolio menu-item-has-children menu-item-1427"><a href="https://futureoflife.org/background/existential-risk/">Existential Risk</a>
<ul class="sub-menu">
<li id="menu-item-1430" class="menu-item menu-item-type-post_type menu-item-object-portfolio menu-item-1430"><a href="https://futureoflife.org/background/benefits-risks-of-artificial-intelligence/">Artificial Intelligence</a></li>
<li id="menu-item-1431" class="menu-item menu-item-type-post_type menu-item-object-portfolio menu-item-1431"><a href="https://futureoflife.org/background/risk-of-biotechnology/">Biotechnology</a></li>
<li id="menu-item-1428" class="menu-item menu-item-type-post_type menu-item-object-portfolio menu-item-1428"><a href="https://futureoflife.org/background/the-risk-of-nuclear-weapons/">Nuclear Weapons</a></li>
<li id="menu-item-1429" class="menu-item menu-item-type-post_type menu-item-object-portfolio menu-item-1429"><a href="https://futureoflife.org/background/climate-change/">Climate Change</a></li>
</ul>
</li>
<li id="menu-item-83" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-83"><a href="https://futureoflife.org/get-involved/">Get Involved</a>
<ul class="sub-menu">
<li id="menu-item-1766" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1766"><a href="https://futureoflife.org/job-postings/">Job Postings</a></li>
</ul>
</li>
<li id="menu-item-39" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-39"><a href="https://futureoflife.org/contact/">Contact</a></li>
</ul></nav> </div>
</div>
<div id='header_main' class='container_wrap container_wrap_logo'>
<div class='container av-logo-container'><div class='inner-container'>
<div id="headergraphics">
<div class="tagline"> Technology is giving life<br>the potential to flourish<br>like never before... </div>
<img src="/wp-content/themes/enfold/images/dead_tree.png?x57718" alt="dead tree image">
<div class="tagline"> ...or to self-destruct.<br>Let's make a difference! </div>
</div><strong class='logo'><a href='https://futureoflife.org/'><img height='100' width='300' src='https://futureoflife.org/wp-content/uploads/2015/10/FLI_logo-1.png?x57718' alt='Future of Life Institute' /></a></strong></div></div><div id='header_main_alternate' class='container_wrap'><div class='container'><nav class='main_menu' data-selectname='Select a page' role="navigation" itemscope="itemscope" itemtype="https://schema.org/SiteNavigationElement"><div class="avia-menu av-main-nav-wrap"><ul id="avia-menu" class="menu av-main-nav"><li id="menu-item-723" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-top-level menu-item-top-level-1"><a href="https://futureoflife.org/front-page-3/" itemprop="url"><span class="avia-bullet"></span><span class="avia-menu-text">News:</span><span class="avia-menu-fx"><span class="avia-arrow-wrap"><span class="avia-arrow"></span></span></span></a></li>
<li id="menu-item-280" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-top-level menu-item-top-level-2"><a href="https://futureoflife.org/ai-news/" itemprop="url"><span class="avia-bullet"></span><span class="avia-menu-text">AI</span><span class="avia-menu-fx"><span class="avia-arrow-wrap"><span class="avia-arrow"></span></span></span></a></li>
<li id="menu-item-537" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-top-level menu-item-top-level-3"><a href="https://futureoflife.org/biotechnology-news/" itemprop="url"><span class="avia-bullet"></span><span class="avia-menu-text">Biotech</span><span class="avia-menu-fx"><span class="avia-arrow-wrap"><span class="avia-arrow"></span></span></span></a></li>
<li id="menu-item-543" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-top-level menu-item-top-level-4"><a href="https://futureoflife.org/nuclear/" itemprop="url"><span class="avia-bullet"></span><span class="avia-menu-text">Nuclear</span><span class="avia-menu-fx"><span class="avia-arrow-wrap"><span class="avia-arrow"></span></span></span></a></li>
<li id="menu-item-542" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-top-level menu-item-top-level-5"><a href="https://futureoflife.org/environment/" itemprop="url"><span class="avia-bullet"></span><span class="avia-menu-text">Climate</span><span class="avia-menu-fx"><span class="avia-arrow-wrap"><span class="avia-arrow"></span></span></span></a></li>
<li id="menu-item-1409" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-top-level menu-item-top-level-6"><a href="https://futureoflife.org/news-from-our-partner-organizations/" itemprop="url"><span class="avia-bullet"></span><span class="avia-menu-text">Partner Orgs</span><span class="avia-menu-fx"><span class="avia-arrow-wrap"><span class="avia-arrow"></span></span></span></a></li>
<li id="menu-item-search" class="noMobile menu-item menu-item-search-dropdown menu-item-avia-special">
<a href="?s=" data-avia-search-tooltip="

&lt;form action=&quot;https://futureoflife.org/&quot; id=&quot;searchform&quot; method=&quot;get&quot; class=&quot;&quot;&gt;
	&lt;div&gt;
		&lt;input type=&quot;submit&quot; value=&quot;&quot; id=&quot;searchsubmit&quot; class=&quot;button avia-font-entypo-fontello&quot; /&gt;
		&lt;input type=&quot;text&quot; id=&quot;s&quot; name=&quot;s&quot; value=&quot;&quot; placeholder='Search' /&gt;
			&lt;/div&gt;
&lt;/form&gt;" aria-hidden='true' data-av_icon='' data-av_iconfont='entypo-fontello'><span class="avia_hidden_link_text">Search</span></a>
</li></ul></div></nav></div> </div>

</div>
<div class='header_bg'></div>

</header>
<div id='main' class='all_colors' data-scroll-offset='116'>
<div class='main_color container_wrap_first container_wrap sidebar_right'><div class='container'><main role="main" itemprop="mainContentOfPage" class='template-page content  av-content-small alpha units'><div class='post-entry post-entry-type-page post-entry-875'><div class='entry-content-wrapper clearfix'><section class="av_textblock_section" itemscope="itemscope" itemtype="https://schema.org/CreativeWork"><div class='avia_textblock ' itemprop="text"><h2>2015 Project Grants Recommended for Funding</h2>
<table class="grant_recipients" border="3" cellspacing="0" cellpadding="5">
<tbody>
<tr>
<td>Primary Investigator</td>
<td>Project Title</td>
<td>Amount Recommended</td>
<td>Email</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Aiken">Alex Aiken, Stanford University</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Aiken">Verifying Deep Mathematical Properties of AI Systems</a></td>
<td>$100,813</td>
<td>aiken@cs.stanford.edu</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Asaro">Peter Asaro, The New School</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Asaro">Regulating Autonomous Artificial Agents: A Systematic Approach to Developing AI &amp; Robot Policy</a></td>
<td>$116,974</td>
<td>peterasaro@gmail.com</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Baum">Seth Baum, Social &amp; Environmental Entrepreneurs</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Baum">Evaluation of Safe Development Pathways for Artificial Superintelligence</a></td>
<td>$100,000</td>
<td>seth@gcrinstitute.org</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Christiano">Paul Christiano, University of California, Berkeley</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Christiano">Counterfactual Human Oversight</a></td>
<td>$50,000</td>
<td>paulfchristiano@gmail.com</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Conitzer">Vincent Conitzer, Duke University</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Conitzer">How to Build Ethics into Robust Artificial Intelligence</a></td>
<td>$200,000</td>
<td>conitzer@cs.duke.edu</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Cotton">Owen Cotton-Barratt, Centre for Effective Altruism, Oxford</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Cotton">Decision-relevant uncertainty in AI safety</a></td>
<td>$119,670</td>
<td>owen.cb@centreforeffectivealtruism.org</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Dietterich">Thomas Dietterich, Oregon State University</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Dietterich">Robust and Transparent Artificial Intelligence Via Anomaly Detection and Explanation</a></td>
<td>$200,000</td>
<td>tgd@cs.orst.edu</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Ermon">Stefano Ermon, Stanford University</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Ermon">Robust probabilistic inference engines for autonomous agents</a></td>
<td>$250,000</td>
<td>ermon@cs.stanford.edu</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Evans">Owain Evans, University of Oxford</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Evans">Inferring Human Values: Learning “Ought”, not “Is”</a></td>
<td>$227,212</td>
<td>owaine@gmail.com</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Fallenstein">Benja Fallenstein, Machine Intelligence Research Institute</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Fallenstein">Aligning Superintelligence With Human Interests</a></td>
<td>$250,000</td>
<td>benja@intelligence.org</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Grace">Katja Grace, Machine Intelligence Research Institute</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Grace">AI Impacts</a></td>
<td>$49,310</td>
<td>katjasolveig@gmail.com</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Herd">Seth Herd, University of Colorado</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Herd">Stability of Neuromorphic Motivational Systems</a></td>
<td>$98,400</td>
<td>seth.herd@colorado.edu</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Kumar">Ramana Kumar, University of Cambridge</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Kumar">Applying Formal Verification to Reflective Reasoning</a></td>
<td>$36,750</td>
<td>ramana.kumar@gmail.com</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Li">Fuxin Li, Georgia Institute of Technology</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Li">Understanding when a deep network is going to be wrong</a></td>
<td>$121,642</td>
<td>fli@cc.gatech.edu</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Liang">Percy Liang, Stanford University</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Liang">Predictable AI via Failure Detection and Robustness</a></td>
<td>$255,160</td>
<td>pliang@cs.stanford.edu</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Ouyang">Long Ouyang, Theiss Research</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Ouyang">Democratizing Programming: Synthesizing Valid Programs with Recursive Bayesian Inference</a></td>
<td>$99,750</td>
<td>longouyang@post.harvard.edu</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Parkes">David Parkes, Harvard University</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Parkes">Mechanism Design for AI Architectures</a></td>
<td>$200,000</td>
<td>parkes@eecs.harvard.edu</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Platzer">Andre Platzer, Carnegie Mellon University</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Platzer">Faster Verification of AI-based Cyber-physical Systems</a></td>
<td>$200,000</td>
<td>aplatzer@cs.cmu.edu</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Roff">Heather Roff, University of Denver</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Roff">Lethal Autonomous Weapons, Artificial Intelligence and Meaningful Human Control</a></td>
<td>$136,918</td>
<td>Heather.Roff@colorado.edu</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Rossi">Francesca Rossi, University of Padova</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Rossi">Safety Constraints and Ethical Principles in Collective Decision Making Systems</a></td>
<td>$275,000</td>
<td>frossi@math.unipd.it</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Rubinstein">Benjamin Rubinstein, The University of Melbourne</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Rubinstein">Security Evaluation of Machine Learning Systems</a></td>
<td>$98,532</td>
<td>benjamin.rubinstein@unimelb.edu.au</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Russell">Stuart Russell, University of California, Berkeley</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Russell">Value Alignment and Moral Metareasoning</a></td>
<td>$342,727</td>
<td>russell@cs.berkeley.edu</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Selman">Bart Selman, Cornell University</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Selman">Scaling-up AI Systems: Insights From Computational Complexity</a></td>
<td>$24,950</td>
<td>bart.selman@gmail.com</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Sotala">Kaj Sotala, Theiss Research</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Sotala">Teaching AI Systems Human Values Through Human-Like Concept Learning</a></td>
<td>$20,000</td>
<td>kaj.sotala@intelligence.org</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Steunebrink">Bas Steunebrink, IDSIA</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Steunebrink">Experience-based AI (EXPAI)</a></td>
<td>$196,650</td>
<td>bas@idsia.ch</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Veloso">Manuela Veloso, Carnegie Mellon University</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Veloso">Explanations for Complex AI Systems</a></td>
<td>$200,000</td>
<td>mmv@cs.cmu.edu</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Webb">Michael Webb, Stanford University</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Webb">Optimal Transition to the AI Economy</a></td>
<td>$76,318</td>
<td>michaelwebb@gmail.com</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Weld">Daniel Weld, University of Washington</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Weld">Computational Ethics for Probabilistic Planning</a></td>
<td>$200,000</td>
<td>weld@cs.washington.edu</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Weller">Adrian Weller, University of Cambridge</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Weller">Investigation of Self-Policing AI Agents</a></td>
<td>$50,000</td>
<td>aw665@cam.ac.uk</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Wellman">Michael Wellman, University of Michigan</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Wellman">Understanding and Mitigating AI Threats to the Financial System</a></td>
<td>$200,000</td>
<td>wellman@umich.edu</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Wooldridge">Michael Wooldridge, University of Oxford</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Wooldridge">Towards a Code of Ethics for AI Research</a></td>
<td>$125,000</td>
<td>mjw@cs.ox.ac.uk</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Ziebart">Brian Ziebart, University of Illinois at Chicago</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Ziebart">Towards Safer Inductive Learning</a></td>
<td>$134,247</td>
<td>bziebart@uic.edu</td>
</tr>
</tbody>
</table>
<h2>2015 Center Grant Recommendation</h2>
<table class="grant_recipients" border="3" cellspacing="0" cellpadding="5">
<tbody>
<tr>
<td>Primary Investigator</td>
<td>Project Title</td>
<td>Amount Recommended</td>
<td>Email</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Bostrom">Nick Bostrom, University of Oxford</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Bostrom">Strategic Research Center for Artificial Intelligence</a></td>
<td>$1,500,000</td>
<td>nick.bostrom@philosophy.ox.ac.uk</td>
</tr>
</tbody>
</table>
<h2>2015 Conference and Education Grant Recommendations</h2>
<table class="grant_recipients" border="3" cellspacing="0" cellpadding="5">
<tbody>
<tr>
<td>Primary Investigator</td>
<td>Project Title</td>
<td>Amount Recommended</td>
<td>Email</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Wallach">Wendell Wallach, Yale</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Wallach">Control and Responsible Innovation in the Development of Autonomous Machines</a></td>
<td>$180,000</td>
<td>wendell.wallach@yale.edu</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Salamon">Anna Salamon, Center for Applied Rationality</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Salamon">Specialized rationality skills for the AI research community</a></td>
<td>$111,757</td>
<td>anna@rationality.org</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Vardi">Moshe Vardi, Rice University</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Vardi">Artificial Intelligence and the Future of Work</a></td>
<td>$69,000</td>
<td>vardi@cs.rice.edu</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/AI/2015awardees#Steinhardt">Jacob Steinhardt, Stanford University</a></td>
<td><a href="http://futureoflife.org/AI/2015awardees#Steinhardt">Summer Program in Applied Rationality and Cognition</a></td>
<td>$88,050</td>
<td>jacob.steinhardt@gmail.com</td>
</tr>
</tbody>
</table>
<div>
<h2>Project Summaries</h2>
<h3><a class="adjust_anchor" name="Aiken"></a><a href="http://theory.stanford.edu/~aiken/">Alex Aiken</a></h3>
<p><i>Project Summary: </i>Artificial Intelligence (AI) is a broad and open-ended research area, and the risks that AI systems will pose in the future are extremely hard to characterize. However, it seems likely that any AI system will involve substantial software complexity, will depend on advanced mathematics in both its implementation and justification, and will be naturally flexible and seem to degrade gracefully in the presence of many types of implementation errors. Thus we face a fundamental challenge in developing trustworthy AI: how can we build and maintain complex software systems that require advanced mathematics in order to implement and understand, and which are all but impossible to verify empirically? We believe that it will be possible and desirable to formally state and prove that the desired mathematical properties hold with respect to the underlying programs, and to maintain such proofs as part of the software artifacts themselves. We propose to demonstrate the feasibility of this methodology by building a system that takes beliefs about the world in the form of probabilistic models, synthesizes inference algorithms to update those beliefs in the presence of observations, and provides formal proofs that the inference algorithms are correct with respect to the laws of probability.</p>
<p><strong>Technical Abstract: </strong><a href="http://futureoflife.org/static/data/full-proposal-files/abstract.pdf?x57718">here</a></p>
<h3><a class="adjust_anchor" name="Asaro"></a><a href="http://www.newschool.edu/public-engagement/faculty-list/?id=4d6a-5177-4e7a-6b79">Peter Asaro</a></h3>
<p><i>Project Summary: </i>For society to enjoy many of the benefits of advanced artificial intelligence (AI) and robotics, it will be necessary to deal with situations that arise in which autonomous artificial agents violate laws or cause harm. If we want to allow AIs and robots to roam the internet and the physical world and take actions that are unsupervised by humans — as may be necessary for, e.g. personal shopping assistants, self-driving cars, and host of other applications — we must be able to manage the liability for the harms they might cause to individuals and property. Resolving this issue will require untangling a set of theoretical and philosophical issues surrounding causation, intention, agency, responsibility, culpability and compensation, and distinguishing different varieties of agency, such as causal, legal and moral. With a clearer understanding of the central concepts and issues, this project will provide a better foundation for developing policies which will enable society to utilize artificial agents as they become increasingly autonomous, and ensuring that future artificial agents can be both robust and beneficial to society, without stifling innovation.</p>
<p><strong>Technical Abstract: </strong>This project addresses a central issue — “the liability problem” — facing the regulation of artificial computational agents, including artificial intelligence (AI) and robotic systems, as they become increasingly autonomous, and supersede current capabilities. In order for society to benefit from advances in AI technology, it will be necessary to develop regulatory policies which manage the risk and liability of deploying systems with increasingly autonomous capabilities. However, current approaches to liability have difficulties when it comes to dealing with autonomous artificial agents because their behavior may be unpredictable to those who create and deploy them, and they will not be proper legal agents. The project will explore the fundamental concepts of autonomy, agency and liability; clarify the different varieties of agency that artificial systems might realize, including causal, legal and moral; and the illuminate the relationships between these. The project will take a systematic approach by integrating an analysis of fundamental concepts “including autonomy, agency, causation, intention, responsibility and culpability” and their applicability to autonomous artificial agents, surveying current legal approaches to liability, and exploring possible approaches for future regulatory policy. It will deliver a book-length publication containing the theoretical research results and recommendations for policy-making.</p>
<h3><a class="adjust_anchor" name="Baum"></a><a href="http://gcrinstitute.org/author/seth-baum/">Seth Baum</a></h3>
<p><i>Project Summary: </i>Some experts believe that computers could eventually become a lot smarter than humans are. They call it artificial superintelligence, or ASI. If people build ASI, it could be either very good or very bad for humanity. However, ASI is not well understood, which makes it difficult for people to act to enable good ASI and avoid bad ASI. Our project studies the ways that people could build ASI in order to help people act in better ways. We will model the different steps that need to occur for people to build ASI. We will estimate how likely it is that these steps will occur, and when they might occur. We will also model the actions people can take, and we will calculate how much the actions will help. For example, governments may be able to require that ASI researchers build in safety measures. Our models will include both the government action and the ASI safety measures, to learn about how well it all works. This project is an important step towards making sure that humanity avoids bad ASI and, if it wishes, creates good ASI.</p>
<p><strong>Technical Abstract: </strong>Artificial superintelligence (ASI) has been proposed to be a major transformative future technology, potentially resulting in either massive improvement in the human condition or existential catastrophe. However, the opportunities and risks remain poorly characterized and quantified. This reduces the effectiveness of efforts to steer ASI development towards beneficial outcomes and away from harmful outcomes. While deep uncertainty inevitably surrounds such a breakthrough future technology, significant progress can be made now using available information and methods. We propose to model the human process of developing ASI. ASI would ultimately be a human creation; modeling this process indicates the probability of various ASI outcomes and illuminates a range of ways to improve outcomes. We will characterize the development pathways that can result in beneficial or dangerous ASI outcomes. We will apply risk analysis and decision analysis methods to quantify opportunities and risks, and to evaluate opportunities to make ASI less risky and more beneficial. Specifically, we will use fault trees and influence diagrams to map out ASI development pathways and the influence that various actions have on these pathways. Our proposed project will produce the first-ever analysis of ASI development using rigorous risk and decision analysis methodology.</p>
<h3><a class="adjust_anchor" name="Christiano"></a><a href="http://paulfchristiano.com/">Paul Christiano</a></h3>
<p><i>Project Summary: </i>Autonomous goal-directed systems may behave flexibly with minimal human involvement. Unfortunately, such systems could also be dangerous if pursuing an incorrect or incomplete goal.</p>
<p>Meaningful human control can ensure that each decision ultimately reflects the desires of a human operator, with AI systems merely providing capabilities and advice. Unfortunately, as AI becomes more capable such control becomes increasingly limiting and expensive.</p>
<p>I propose to study an intermediate approach, where a system’s behavior is shaped by what a human operator would have done if they had been involved, rather than either requiring actual involvement or pursuing a goal without any oversight. This approach may be able to combine the safety of human control with the efficiency of autonomous operation. But capturing either of these benefits requires confronting new challenges: to be safe, we must ensure that our AI systems do not cause harm by incorrectly predicting the human operator; to be efficient and flexible, we must enable the human operator to provide meaningful oversight in domains that are too complex for them to reason about unaided. This project will study both of these problems, with the goal of designing concrete mechanisms that can realize the promise of this approach.</p>
<p><strong>Technical Abstract: </strong><a href="http://futureoflife.org/static/data/full-proposal-files/abstract_1.pdf?x57718">here</a></p>
<h3><a class="adjust_anchor" name="Conitzer"></a><a href="http://www.cs.duke.edu/~conitzer/">Vincent Conitzer</a></h3>
<p><i>Project Summary: </i>Humans take great pride in being the only creatures who make moral judgments, even though their moral judgments often suffer from serious flaws. Some AI systems do generate decisions based on their consequences, but consequences are not all there is to morality. Moral judgments are also affected by rights (such as privacy), roles (such as in families), past actions (such as promises), motives and intentions, and other morally relevant features. These diverse factors have not yet been built into AI systems. Our goal is to do just that. Our team plans to combine methods from computer science, philosophy, and psychology in order to construct an AI system that is capable of making plausible moral judgments and decisions in realistic scenarios. We hope that this work will provide a basis that leads to future highly-advanced AI systems acting ethically and thereby being more robust and beneficial. Humans, by comparing their own moral judgments to the output of the resulting system, will be able to understand their own moral judgments and avoid common mistakes (such as partiality and overlooking relevant factors). In these ways and more, moral AI might also make humans more moral.</p>
<p><strong>Technical Abstract: </strong>Most contemporary AI systems base their decisions solely on consequences, whereas humans also consider other morally relevant factors, including rights (such as privacy), roles (such as in families), past actions (such as promises), motives and intentions, and so on. Our goal is to build these additional morally relevant features into an AI system. We will identify morally relevant features by reviewing theories in moral philosophy, conducting surveys in moral psychology, and using machine learning to locate factors that affect human moral judgments. We will use and extend game theory and social choice theory to determine how to make these features more precise, how to weigh conflicting features against each other, and how to build these features into an AI system. We hope that eventually this work will lead to highly advanced AI systems that are capable of making moral judgments and acting on them. Humans will then be able to compare these outputs to their own moral judgments in order to learn which of these judgments are distorted by biases, partiality, or lack of attention to relevant factors. In such ways, moral AI can also contribute to our own understanding of morality and our moral lives.</p>
<h3><a class="adjust_anchor" name="Cotton"></a><a href="http://www.oxfordmartin.ox.ac.uk/people/655">Owen Cotton-Barratt</a></h3>
<p><i>Project Summary: </i>What are the most important projects for reducing the risk of harm from superintelligent artificial intelligence? We will probably not have to deal with such systems for many years – and we do not expect they will be developed with the same architectures we use today. That may make us want to focus on developing long-term capabilities in AI safety research. On the other hand, there are forces pushing us towards working on near-term problems. We suffer from ‘near-sightedness’ and are better at finding the answer to questions that are close at hand. Just as important, work on long-term problems can happen in the future and get extra people attending to it, while work on near-term problems has to happen now if it is to happen at all.</p>
<p>This project models the trade-offs we make when carrying out AI safety projects that aim at various horizons, and focused on specific architectures. It estimates crucial parameters – like the time-horizon probability distribution and how near-sighted we tend to be. It uses that model to work out what the AI safety community should be funding, and what it should call on policymakers to do.</p>
<p><strong>Technical Abstract: </strong><a href="http://futureoflife.org/static/data/full-proposal-files/Global_Priorities_Project_-_3.pdf?x57718">here</a></p>
<h3><a class="adjust_anchor" name="Dietterich"></a><a href="http://web.engr.oregonstate.edu/~tgd/">Thomas Dietterich</a></h3>
<p><i>Project Summary: </i>In the early days of AI research, scientists studied problems such as chess and theorem proving that involved “micro worlds” that were perfectly known and predictable. Since the 1980s, AI researchers have studied problems involving uncertainty. They apply probability theory to model uncertainty about the world and use decision theory to represent the utility of the possible outcomes of proposed actions. This allows computers to make decisions that maximize expected utility by taking into account the “known unknowns”. However, when such AI systems are deployed in the real world, they can easily be confused by “unknown unknowns” and make poor decisions. This project will develop theoretical principles and AI algorithms for learning and acting safely in the presence of unknown unknowns. The algorithms will be able to detect and respond to unexpected changes in the world. They will ensure that when the AI system plans a sequence of actions, it takes into account its ignorance of the unknown unknowns. This will lead it to behave cautiously and turn to humans for help. Instead of maximizing expected utility, it will first ensure that its actions avoid unsafe outcomes and only then maximize utility. This will make AI systems much safer.</p>
<p><strong>Technical Abstract: </strong><a href="http://futureoflife.org/static/data/full-proposal-files/abstract-v1.pdf?x57718">here</a></p>
<h3><a class="adjust_anchor" name="Ermon"></a><a href="http://cs.stanford.edu/~ermon/">Stefano Ermon</a></h3>
<p><i>Project Summary: </i>As we close the loop between sensing-reasoning-acting, autonomous agents such as self-driving cars are required to act intelligently and adaptively in increasingly complex and uncertain real-world environments. To make sensible decisions under uncertainty, agents need to reason probabilistically about their environments, e.g., estimate the probability that a pedestrian will cross or that a car will change lane. Over the past decades, AI research has made tremendous progress in automated reasoning. Existing technology achieves super-human performance in numerous domains, including chess-playing and crossword-solving. Unfortunately, current approaches do not provide worst-case guarantees on the quality of the results obtained. For example, it is not possible to rule out completely unexpected behaviors or catastrophic failures. Therefore, we propose to develop novel reasoning technology focusing on soundness and robustness. This research will greatly improve the reliability and safety of next-generation autonomous agents.</p>
<p><strong>Technical Abstract: </strong>To cope with the uncertainty and ambiguity of real world domains, modern AI systems rely heavily on statistical approaches and probabilistic modeling. Intelligent autonomous agents need to solve numerous probabilistic reasoning tasks, ranging from probabilistic inference to stochastic planning problems. Safety and reliability depend crucially on having both accurate models and sound reasoning techniques. To date, there are two main paradigms for probabilistic reasoning: exact decomposition-based techniques and approximate methods such as variational and MCMC sampling. Neither of them is suitable for supporting autonomous agents interacting with complex environments safely and reliably. Decomposition-based techniques are accurate but are not scalable. Approximate techniques are more scalable, but in most cases do not provide formal guarantees on the accuracy. We therefore propose to develop probabilistic reasoning technology which is both scalable and provides formal guarantees, i.e., “certificates” of accuracy, as in formal verification. This research will bridge probabilistic and deterministic reasoning, drawing from their respective strengths, and has the potential to greatly improve the reliability and safety of AI and cyber-physical systems.</p>
<h3><a class="adjust_anchor" name="Evans"></a><a href="http://web.mit.edu/owain/www/">Owain Evans</a></h3>
<p><i>Project Summary: </i>Previous work in economics and AI has developed mathematical models of preferences or values, along with computer algorithms for inferring preferences from observed human choices. We would like to use such algorithms to enable AI systems to learn human preferences by observing humans make real-world choices. However, these algorithms rely on an assumption that humans make optimal plans and take optimal actions in all circumstances. This is typically false for humans. For example, people’s route planning is often worse than Google Maps, because we can’t number-crunch as many possible paths. Humans can also be inconsistent over time, as we see in procrastination and impulsive behavior. Our project seeks to develop algorithms that learn human preferences from data despite humans not being homo-economicus and despite the influence of non-rational impulses. We will test our algorithms on real-world data and compare their inferences to people’s own judgments about their preferences. We will also investigate the theoretical question of whether this approach could enable an AI to learn the entirety of human values.</p>
<p><strong>Technical Abstract: </strong>Previous work in economics and AI has developed mathematical models of preferences, along with algorithms for inferring preferences from observed actions. We would like to use such algorithms to enable AI systems to learn human preferences from observed actions. However, these algorithms typically assume that agents take actions that maximize expected utility given their preferences. This assumption of optimality is false for humans in real-world domains. Optimal sequential planning is intractable in complex environments and humans perform very rough approximations. Humans often don’t know the causal structure of their environment (in contrast to MDP models). Humans are also subject to dynamic inconsistencies, as observed in procrastination, addiction and in impulsive behavior. Our project seeks to develop algorithms that learn human preferences from data despite the suboptimality of humans and the behavioral biases that influence human choice. We will test our algorithms on real-world data and compare their inferences to people’s own judgments about their preferences. We will also investigate the theoretical question of whether this approach could enable an AI to learn the entirety of human values.</p>
<h3><a class="adjust_anchor" name="Fallenstein"></a><a href="https://intelligence.org/team/">Benja Fallenstein</a></h3>
<p><i>Project Summary: </i>How can we ensure that powerful AI systems of the future behave in ways that are reliably aligned with human interests?</p>
<p>One productive way to begin study of this AI alignment problem in advance is to build toy models of the unique safety challenges raised by such powerful AI systems and see how they behave, much as Konstantin Tsiolkovsky wrote down (in 1903) a toy model of how a multistage rocket could be launched into space. This enabled Tsiolkovsky and others to begin exploring the specific challenges of spaceflight long before such rockets were built.</p>
<p>Another productive way to study the AI alignment problem in advance is to seek formal foundations for the study of well-behaved powerful Ais, much as Tsiolkovsky derived the rocket equation (also in 1903) which governs the motion of rockets under ideal environmental conditions. This was a useful stepping stone toward studying the motion of rockets in actual environments.</p>
<p>We plan to build toy models and seek formal foundations for many aspects of the AI alignment problem. One example is that we aim to improve our toy models of a corrigible agent which avoids default rational incentives to resist its programmers’ attempts to fix errors in the AI’s goals.</p>
<p><strong>Technical Abstract: </strong>The Future of Life Institute’s research priorities document calls for research focused on ensuring beneficial behavior in [AI] systems that can learn from experience with human-like breadth and surpass human performance in most cognitive tasks. We aim to study several sub-problems of this ‘AI alignment problem, by illuminating the key difficulties using toy models, and by seeking formal foundations for robustly beneficial intelligent agents. In particular, we hope to (a) improve our toy models of ‘corrigible agents’ which avoid default rational incentives to resist corrective interventions from the agents’ programmers, (b) continue our preliminary efforts to put formal foundations under the study of naturalistic, embedded agents which avoid the standard agent-environment split currently used as a simplifying assumption throughout the field of AI, and (c) continue our preliminary efforts to overcome obstacles to flexible cooperation in multi-agent settings. We also hope to take initial steps in formalizing several other informal problems related to AI alignment, for example the problem of ‘ontology identification’: Given goals specified with respect to some ontology and a world model, how can the ontology of the goals be identified inside the world model?</p>
<h3><a class="adjust_anchor" name="Grace"></a><a href="http://katjagrace.com/">Katja Grace</a></h3>
<p><i>Project Summary: </i>Many experts think that within a century, artificial intelligence will be able to do almost anything a human can do. This might mean humans are no longer in control of what happens, and very likely means they are no longer employable. The world might be very different, and the changes that take place could be dangerous.</p>
<p>Very little research has asked when this transition will happen, what will happen, and how we can make it go well. AI Impacts is a project to ask those questions, and to answer them rigorously. We look for research projects that can shed light on the future of AI; especially on questions that matter to people making decisions. We publish the results online, and explain our research to a broad audience.</p>
<p>We are currently working on comparing the power of the brain to that of supercomputers, to help calculate when people will have enough hardware to run something as complex as a brain. We are also checking whether AI progress is likely to see sudden jumps, by looking for jumps in other areas of technological progress.</p>
<p><strong>Technical Abstract: </strong>‘Human-level’ artificial intelligence will have far-reaching effects on society, and is generally anticipated within the coming century. Relatively little is known about the timelines or consequences of this arrival, though increasingly many decisions depend on guesses about it. AI Impacts identifies cost-effective research projects which might shed light on the future of AI, and especially on the parts of it that might guide policy and other decisions. We perform a selection of these research projects, and publish the results as accessible articles in the public domain.</p>
<p>We recently made a preliminary estimate of the computing performance of the brain in terms of traversed edges per second (TEPS), “a supercomputing benchmark” to better judge when computing hardware will be capable of replicating what the brain does, given the right software. We are also collecting case studies of abrupt technological progress to aid in evaluating the probability of discontinuities in AI progress. In the coming year we will continue with both of these projects, publish articles about several projects in progress, and start several new projects.</p>
<h3><a class="adjust_anchor" name="Herd"></a><a href="http://psych.colorado.edu/~cognitive/co-researchers.html#herd">Seth Herd</a></h3>
<p><i>Project Summary: </i>We are investigating the safety of possible future advanced AI that uses the same basic approach to motivated behavior as that used by the human brain. Neuroscience has given us a rough blueprint of how the brain directs its behavior based on its innate motivations and its learned goals and values. This blueprint may be used to guide advances in artificial intelligence to produce AI that is as intelligent and capable as humans, and soon after, more intelligent. While it is impossible to predict how long this progress might take, it is also impossible to predict how quickly it might happen. Rapidly progress in practical applications is producing rapid increases in funding from commercial and governmental sources. Thus, it seems critical to understand the potential risks of brain-style artificial intelligence before it is actually achieved. We are testing our model of brain-style motivational systems in a highly simplified environment, to investigate how its behavior may change as it learns and becomes more intelligent. While our system is not capable of performing useful tasks, it serves to investigate the stability of such systems when they are integrated with powerful learning systems currently being developed and deployed.</p>
<p><strong>Technical Abstract: </strong>We apply a neural network model of human motivated decision-making to an investigation of the risks involved in creating artificial intelligence with a brain-style motivational system. This model uses relatively simple principles to produce complex, goal-directed behavior. Because of the potential utility of such a system, we believe that this approach may see common adoption, and has significant risks. Such a system could provide the motivational core of efforts to create artificial general intelligence (AGI). Such a system has the advantage of leveraging the wealth of knowledge already available and rapidly accumulating on the neuroscience of mammalian motivation and self-directed learning. We employ this model, and non-biological variations on it, to investigate the risks of employing such systems in combination with powerful learning mechanisms that are currently being developed. We investigate the issues of motivational and representational drift. Motivational drift captures how a system will change the motivations it is initially given and trained on. Representational drift refers to the possibility that sensory and conceptual representations will change over the course of training. We investigate whether learning in these systems can be used to produce a system that remains stable and safe for humans as it develops greater intelligence.</p>
<h3><a class="adjust_anchor" name="Kumar"></a><a href="http://www.cl.cam.ac.uk/~rk436/">Ramana Kumar</a></h3>
<p><i>Project Summary: </i>One path to significantly smarter-than-human artificial agents involves self-improvement, i.e., agents doing artificial intelligence research to make themselves even more capable. If such an agent is designed to be robust and beneficial, it should only execute self-modifying actions if it knows they are improvements, which, at a minimum, means being able to trust that the modified agent only takes safe actions. However, trusting the actions of a similar or smarter agent can lead to problems of self-reference, which can be seen as sophisticated versions of the liar paradox (which shows that the self-referential sentence “this sentence is false” cannot be consistently true or false). Several partial solutions to these problems have recently been proposed. However, current software for formal reasoning does not have sufficient support for self-referential reasoning to make these partial solutions easy to implement and study. In this project, we will implement a toy model of agents using these partial solutions to reason about self-modifications, in order to improve our understanding of the challenges of implementing self-referential reasoning, and to stimulate work on tools suitable for it.</p>
<p><strong>Technical Abstract: </strong>Artificially intelligent agents designed to be highly reliable are likely to include a capacity for formal deductive reasoning to be applied in appropriate situations, such as when reasoning about computer programs including other agents and future versions of the same agent. However, it will not always be possible to model other agents precisely: considering more capable agents, only abstract reasoning about their architecture is possible. Abstract reasoning about the behavior of agents that justify their actions with proofs lead to problems of self-reference and reflection: Godel’s second incompleteness theorem shows that no sufficiently strong proof system can prove its own consistency, making it difficult for agents to show that actions their successors have proven to be safe are in fact safe (since an inconsistent proof system would be able to prove any action “safe”). Recently, some potential approaches to circumventing this obstacle have been proposed in the form of pen-and-paper proofs.</p>
<p>We propose building and studying implementations of agents using these approaches, to better understand the challenges of implementing tools that are able to support this type of reasoning, and to stimulate work in the interactive theorem proving community on this kind of tools.</p>
<h3><a class="adjust_anchor" name="Li"></a><a href="http://www.cc.gatech.edu/~fli/">Fuxin Li</a></h3>
<p><i>Project Summary: </i>Deep learning architectures have fundamentally changed the capabilities of machine learning and benefited many applications such as computer vision, speech recognition, natural language processing, with many more influences to other problems coming along. However, very little is understood about those networks. Months of manual tuning is required for obtaining excellent performance, and the trained networks are often not robust: recent studies have shown that the error rate increases significantly with just slight pixel-level perturbations in image that are not even perceivable by human eyes.</p>
<p>In this proposal, The PI propose to thoroughly study the optimization and robustness of deep convolutional networks in visual object recognition, in order to gain more understanding about deep learning. This includes training procedures that will make deep learning more automatic and lead to less failures in training, as well as confidence estimates when the deep network is utilized to predict on new data. The confidence estimates can be used to control the behavior of a robot employing deep learning so that it will not go on to perform maneuvers that could be dangerous because of erroneous predictions. Understanding these aspects would also be helpful in designing potentially more robust networks in the future.</p>
<p><strong>Technical Abstract: </strong>This work will focus on predicting whether a deep convolutional neural network (CNN) has succeeded. This includes two aspects, first, to find an explanation of why and when can the stochastic optimization in a deep CNN succeed without overfitting and obtain high accuracy. Second, to establish an estimate of confidence of the predictions of the deep learning architecture. Those estimates of confidence can be used as safeguards when utilizing those networks in real life. In order to establish those estimates, this work proposes to start from intuitions drawn from empirical analyses from the training procedure and model structures of deep learning. In-depth analyses will be completed for the mini-batch training procedure and model structures, by illustrating the differences each mini-batch size provides for the training, as well as the low-dimensional manifold structure in the classification. From those analyses, this work will result in approaches to design and control a proper training procedure with less human intervention, as well as confidence estimates by estimating the distance of the testing data to the submanifold that the trained network is effective on.</p>
<h3><a class="adjust_anchor" name="Liang"></a><a href="http://cs.stanford.edu/~pliang/">Percy Liang</a></h3>
<p><i>Project Summary: </i>In order for AI to be safely deployed, the desired behavior of the AI system needs to be based on well-understood, realistic, and empirically testable assumptions. From the perspective of modern machine learning, there are three main barriers to this goal. First, existing theory and algorithms mainly focus on fitting the observable outputs in the training data, which could lead, for instance, to an autonomous driving system that performs well on validation tests but does not understand the human values underlying the desired outputs. Second, existing methods are designed to handle a single specified set of testing conditions, and thus little can be said about how a system will behave in a fundamentally new setting; e.g., an autonomous driving system that performs well in most conditions may still perform arbitrarily poorly during natural disasters. Finally, most systems have no way of detecting whether their underlying assumptions have been violated: they will happily continue to predict and act even on inputs that are completely outside the scope of the system.</p>
<p>In this proposal, we detail a research program for addressing all three of the problems above. Just as statistical learning theory (e.g., the work of Vapnik) laid down the foundations of existing machine learning and AI techniques, allowing the field to flourish over the last 25 years, we aim to lay the groundwork for a new generation of safe-by-design AI systems, which can sustain the continued deployment of AI in society</p>
<p><strong>Technical Abstract: </strong><a href="http://futureoflife.org/static/data/full-proposal-files/abstract_4.pdf?x57718">here</a></p>
<h3><a class="adjust_anchor" name="Ouyang"></a><a href="http://zx.gd/academic/">Long Ouyang</a></h3>
<p><i>Project Summary: </i>One goal of artificial intelligence is <i>valid</i> behavior: computers should perform tasks that people actually want them to do. The current model of programming hinders validity, largely because it focuses on the minutae of <i>how</i> to compute rather than the goal of <i>what</i> to compute. An alternative model offers hope for validity: program synthesis. Here, the user specifies <i>what</i> by giving a small description of their goal (e.g., input-output examples). The synthesizer then infers candidate programs matching that description, which the user selects from.</p>
<p>One shortcoming of synthesizers is that they are <i>truthful</i> rather than <i>helpful:</i> they return answers that are literally consistent with user requirements but no more (e.g., a requirement of “word that starts with the letter a” might return just “a”). By contrast, human read more deeply into requirements, divining the underlying intentions. Helpfulness of this kind has been intensely studied in the linguistic field called <i>pragmatics</i>. This project will investigate how recent developments into computational modeling of pragmatics can be leveraged to improve program synthesis, making it easier to write programs that do what we want with little to no special knowledge.</p>
<p><strong>Technical Abstract: </strong><a href="http://futureoflife.org/static/data/full-proposal-files/abstract_3.pdf?x57718">here</a></p>
<h3><a class="adjust_anchor" name="Parkes"></a><a href="http://www.eecs.harvard.edu/~parkes/">David Parkes</a></h3>
<p><i>Project Summary: </i>Economics models the behavior of people, firms, and other decision makers, as a means to understand how these decisions shape the pattern of activities that produce value and ultimately satisfy (or fail to satisfy) human needs and desires. The field adopts rational models of behavior, either of individuals or of behavior in the aggregate.</p>
<p>Artificial Intelligence (AI) research is also drawn to rationality concepts, which provide an ideal for the computational agents that it seeks to create. Although perfect rationality is not achievable, the capabilities of AI are rapidly advancing, and AI can already surpass human-level capabilities in narrow domains.</p>
<p>We envision a future with a massive number of AIs, these AIs owned, operated, designed, and deployed by a diverse array of entitites. This multiplicity of interacting AIs, apart or together with people, will constitute a social system, and as such economics can provide a useful framework for understanding and influencing the aggregate. In turn, systems populated by AIs can benefit from explicit design of the frameworks within which AIs exist. The proposed research looks to apply the economic theory of mechanism design to the coordination of behavior in systems of multiple AIs, looking to promote beneficial outcomes.</p>
<p><strong>Technical Abstract: </strong>When a massive number of AIs are owned, operated, designed, and deployed by a diverse array of firms, individuals, and governments, this multi-agent AI constitutes a social system, and economics provides a useful framework for understanding and influencing the aggregate. In particular, we need to understand how to design multi-agent systems that promote beneficial outcomes when AIs interact with each other. A successful theory must consider both incentives and privacy considerations.</p>
<p>Mechanism design theory from economics provides a framework for the coordination of behavior, such that desirable outcomes are promoted and less desirable outcomes made less likely because they are not in the self-interest of individual actors. We propose a program of fundamental research to understand the role of mechanism design, multi-agent dynamical models, and privacy-preserving algorithms, especially in the context of multi-agent systems in which the AIs are built through reinforcement learning (RL). The proposed research considers two concrete AI problems: the first is experiment design, typically formalized as a multi-armed bandit process, which we study in a multi-agent, privacy-preserving setting. The second is the more general problem of learning to act in Markovian dynamical systems, including both planning and RL agents.</p>
<h3><a class="adjust_anchor" name="Platzer"></a><a href="http://symbolaris.com/">Andre Platzer</a></h3>
<p><i>Project Summary: </i>The most exciting and impactful uses of artificial intelligence (AI) that will affect everybody in crucial ways involve smart decision making to help people in the real world, such as when driving cars or flying aircraft, or help from robots engaging with humans. All of these have a huge potential for making the world a better place but also impose considerable responsibility on the system designer to ensure it will not do more harm than good because its decisions are sometimes systematically unsafe. Responsibly allowing mankind to rely on such technology requires stringent assurance of its safety. A nontrivial enterprise for the decision flexibility in AI. The goal of this research project is to develop formal verification and validation technology that helps ensure safety, robustness, and reliability of AI-based system designs. The world is an uncertain place. So perfect performance cannot always be guaranteed in all respects. But good system designs do not compromise safety when their performance degrades. The PI is proposing to advance his verification tool KeYmaera that has been used successfully for systems known as cyber-physical systems (combining computer decisions with physics or motion) toward the additional challenges that a deep integration of AI into those systems provides.</p>
<p><strong>Technical Abstract: </strong>The most important and most impactful AI-based systems are those that directly interface with the physical world. Cyber-physical systems (CPS) combine computers (for decisions) and physics (motion) and play a prominent role, e.g., in cars, aircraft, robots. Due to their impact on people, they come with stringent safety requirements in order to make sure they make the world a better place. In order to enable sophisticated automation for these systems, AI-based systems become more prominent but their impact on the safety of the system is not understood well so far.</p>
<p>This project studies ways of extending safety analysis and verification technology for cyber-physical systems with ways of addressing the additional challenges that AI-based CPS provide.</p>
<p>The PI developed a verification tool, KeYmaera, for CPS, which has had quite some success in verifying CPS. KeYmaera has been used successfully for a safety analysis of an AI-intensive CPS, the Airborne Collision Avoidance System ACAS X, albeit with non-negligible effort. For a system of such a world-wide impact as ACAS X, the effort amortizes. This project proposes to develop verification technology that reduces the effort needed to verify AI-based systems in order to achieve more widespread adoption of safety analysis for AI-based CPS.</p>
<h3><a class="adjust_anchor" name="Roff"></a><a href="http://www.du.edu/korbel/faculty/roffperkins.html">Heather Roff</a></h3>
<p><i>Project Summary: </i>There is a growing concern over the deployment of autonomous weapons systems, and how the partnering of artificial intelligence (AI) and weapons will change the future of conflict. The United Nations recently took up the subject of autonomous weapons, and many governments and key international organizations are arguing that such systems require meaningful human control to be acceptable. However, what is human control, and how do we ensure that it is meaningful? This project helps the international community, scholars and practitioners by providing answers those questions and helping to protect the essential elements of human control over the application of force. Bringing together computer scientists, roboticists, ethicists, lawyers and diplomats, the project will produce a conceptual framework that can shape new research and international policy for the future. Moreover, it will create a freely downloadable dataset on existing and emerging semi-autonomous weapons. Through this data, we can gain clarity on how and where autonomous functions are already deployed and on how such functions are kept under human control. A focus on current and emerging technologies makes it clear that the relationship between AI and weapons is not a problem for the distant future, but is a pressing issue now.</p>
<p><strong>Technical Abstract: </strong>The project addresses the relationships between artificial intelligence (AI), weapons systems and society. In particular, the project provides a framework for meaningful human control (MHC) of autonomous weapons systems. In international discussions, a number of governments and organizations adopted MHC as a tool for approaching problems and potential solutions raised by autonomous weapons. However, the content of MHC was left open. While useful for policy reasons, the international community, academics and practioners are calling for further work on this issue. This project responds to that call by bringing together a multidisciplinary and multi-stakeholder team to address key questions. For example, we question the values associated with MHC, what rules should inform the design of the systems “both in software and hardware” and how existing and currently developing weapons systems advance possible relationships between human control, autonomy and AI. To achieve impact across academic, industry and policy arenas, we will produce academic publications, policy briefs, an open access database on ‘semi-autonomous’ weapons, and will sponsor multi-sector stakeholder discussions on how human values can be maintained as systems develop. Furthermore, the organization Article 36 will channel outputs directly into the international diplomatic community to achieve impact in international legal and policy forums.</p>
<h3><a class="adjust_anchor" name="Rossi"></a><a href="http://www.math.unipd.it/~frossi/">Francesca Rossi</a></h3>
<p><i>Project Summary: </i>The future will see autonomous machines acting in the same environment as humans, in areas as diverse as driving, assistive technology, and health care. Think of self-driving cars, companion robots, and medical diagnosis support systems. We also believe that humans and machines will often need to work together and agree on common decisions. Thus hybrid collective decision making systems will be in great need.</p>
<p>In this scenario, both machines and collective decision making systems should follow some form of moral values and ethical principles (appropriate to where they will act but always aligned to humans’), as well as safety constraints. In fact, humans would accept and trust more machines that behave as ethically as other humans in the same environment. Also, these principles would make it easier for machines to determine their actions and explain their behavior in terms understandable by humans. Moreover, often machines and humans will need to make decisions together, either through consensus or by reaching a compromise. This would be facilitated by shared moral values and ethical principles.</p>
<p>We will study the embedding and learning of safety constraints, moral values, and ethical principles in collective decision making systems for societies of machines and humans.</p>
<p><strong>Technical Abstract: </strong>The future will see autonomous agents acting in the same environment as humans, in areas as diverse as driving, assistive technology, and health care. In this scenario, collective decision making will be the norm. We will study the embedding of safety constraints, moral values, and ethical principles in agents, within the context of hybrid human/agents collective decision making. We will do that by adapting current logic-based modelling and reasoning frameworks, such as soft constraints, CP-nets, and constraint-based scheduling under uncertainty. For ethical principles, we will use constraints specifying the basic ethical “laws”, plus sophisticated prioritised and possibly context-dependent constraints over possible actions, equipped with a conflict resolution engine. To avoid reckless behavior in the face of uncertainty, we will bound the risk of violating these ethical laws. We will also replace preference aggregation with an appropriately developed constraint/value/ethics/preference fusion, an operation designed to ensure that agents’ preferences are consistent with the system’s safety constraints, the agents’ moral values, and the ethical principles of both individual agents and the collective decision making system. We will also develop approaches to learn ethical principles for artificial intelligent agents, as well as predict possible ethical violations.</p>
<h3><a class="adjust_anchor" name="Rubinstein"></a><a href="http://www.bipr.net/">Benjamin Rubinstein</a></h3>
<p><i>Project Summary: </i>Machine Learning and Artificial Intelligence underpin technologies that we rely on daily, from consumer electronics (smart phones), medical implants (continuous blood glucose monitors), websites (Facebook, Google), to the systems that defend critical infrastructure. The very characteristic that makes these systems so beneficial — adaptability — can also be exploited by sophisticated adversaries wishing to breach system security or gain an economic advantage. This project will develop usable software tools for evaluating vulnerabilities in learning systems, a first step towards general-purpose, secure machine learning.</p>
<p><strong>Technical Abstract: </strong>This project aims to develop systems for the analysis of machine learning algorithms in adversarial environments. Today Machine Learning and Statistics are employed in many technologies where participants have an incentive to game the system, for example internet ad placement, cybersecurity, credit risk in finance, health analytics, and smart utility grids. However little is known about how well state-of-the-art inference techniques fare when data is manipulated by a malicious adversary. By formulating the process of evading a learned model, or manipulating training data to poison learning, as an optimization program, our approach to evaluating security reduces to one a projected subgradient descent. Our main method for solving such iterative optimizations generically, will be to employ the dynamic code analysis represented by automatic differentiation. A key output of this project will be usable software tools for evaluating the security of learning systems in general.</p>
<h3><a class="adjust_anchor" name="Russell"></a><a href="https://www.cs.berkeley.edu/~russell/">Stuart Russell</a></h3>
<p><i>Project Summary: </i>Developing AI systems that are benevolent towards humanity requires making sure that those systems know what humans want. People routinely make inferences about the preferences of others and use those inferences as the basis for helping one another. This project aims to provide AI systems a similar ability to learn from observations, in order to better align the values of those systems with those of humans. Doing so requires dealing with some significant challenges: If we ultimately develop AI systems that can reason better than humans, how do we make sure that those AI systems are able to take human limitations into account? The fact that we haven’t yet cured cancer shouldn’t be taken as evidence that we don’t really care about it. Furthermore, once we have made an AI system that can reason about human preferences, that system then has to trade off time spent in deliberating about the right course of action with the need to act as quickly as possible – it needs to deal with its own computational limitations as it makes decisions. We aim to address both these challenges by examining how intelligent agents (be they humans or computers) should make these tradeoffs.</p>
<p><strong>Technical Abstract: </strong><a href="http://futureoflife.org/static/data/full-proposal-files/project_abstract_1.pdf?x57718">here</a></p>
<h3><a class="adjust_anchor" name="Selman"></a><a href="http://www.cs.cornell.edu/selman/">Bart Selman</a></h3>
<p><i>Project Summary: </i>There is general consensus within the AI research community that progress in the field is accelerating: it is believed that human-level AI will be reached within the next one or two decades. A key question is whether these advances will accelerate further after general human level AI is achieved, and, if so, how rapidly the next level of AI systems (?super-human?) will be achieved.</p>
<p>Since the mid 1970s, Computer scientists have developed a rich theory about the computational resources that are needed to solve a wide range of problems. We will use these methods to make predictions about the feasibility of super-human level cognition.</p>
<p><strong>Technical Abstract: </strong>There is general consensus within the AI research community that progress in the field is accelerating: it is believed that human-level AI will be reached within the next one or two decades on a range of cognitive tasks. A key question is whether these advances will accelerate further after general human level AI is achieved, and, if so, how rapidly the next level of AI systems (‘super-human’) will be achieved. Having a better understanding of how rapidly we may reach this next phase will be useful in preparing for the advent of such systems.</p>
<p>Computational complexity theory provides key insights into the scalability of computational systems. We will use methods from complexity theory to analyze the possibility of the scale-up to super-human intelligence and the speed of such scale-up for different categories of cognition.</p>
<h3><a class="adjust_anchor" name="Sotala"></a><a>Kaj Sotala</a></h3>
<p><i>Project Summary: </i>AI systems will need to understand human values in order to respect them. This requires having similar concepts as humans do. We will research whether AI systems can be made to learn their concepts in the same way as humans learn theirs. This will involve a literature review of the relevant fields, as well as experimental work.</p>
<p>We are particularly interested in a branch of machine learning called deep learning. The concepts learned by deep learning agents seem to be similar as the ones that have been documented in psychology. We will attempt to apply existing deep learning methodologies for learning what we call moral concepts, concepts through which moral values are defined. In addition, we will investigate a particular hypothesis of how we develop our concepts and values in the first place.</p>
<p><strong>Technical Abstract: </strong>Autonomous AI systems will need to understand human values in order to respect them. This requires having similar concepts as humans do. We will research whether AI systems can be made to learn their concepts in the same way as humans learn theirs. This will involve a literature review of the relevant fields, as well as experimental work.</p>
<p>Both human concepts and the representations of deep learning models seem to involve a hierarchical structure, among other similarities. For this reason, we will attempt to apply existing deep learning methodologies for learning what we call moral concepts, concepts through which moral values are defined. In addition, we will investigate the extent to which reinforcement learning affects the development of our concepts and values.</p>
<h3><a class="adjust_anchor" name="Steunebrink"></a><a href="http://people.idsia.ch/~steunebrink/">Bas Steunebrink</a></h3>
<p><i>Project Summary: </i>As it becomes ever clearer how machines with a human level of intelligence can be built — and indeed that they will be built — there is a pressing need to discover ways to ensure that such machines will robustly remain benevolent, especially as their intellectual and practical capabilities come to surpass ours. Through self-modification, highly intelligent machines may be capable of breaking important constraints imposed initially by their human designers. The currently prevailing technique for studying the conditions for preventing this danger is based on forming mathematical proofs about the behavior of machines under various constraints. However, this technique suffers from inherent paradoxes and requires unrealistic assumptions about our world, thus not proving much at all.</p>
<p>Recently a class of machines that we call experience-based artificial intelligence (EXPAI) has emerged, enabling us to approach the challenge of ensuring robust benevolence from a promising new angle. This approach is based on studying how a machine’s intellectual growth can be molded over time, as the machine accumulates real-world experience, and putting the machine under pressure to test how it handles the struggle to adhere to imposed constraints.</p>
<p>The Swiss AI lab IDSIA will deliver a widely applicable EXPAI growth control methodology.</p>
<p><strong>Technical Abstract: </strong>Whenever one wants to verify that a recursively self-improving system will robustly remain benevolent, the prevailing tendency is to look towards formal proof techniques, which however have several issues: (1) Proofs rely on idealized assumptions that inaccurately and incompletely describe the real world and the constraints we mean to impose. (2) Proof-based self-modifying systems run into logical obstacles due to Lob’s theorem, causing them to progressively lose trust in future selves or offspring. (3) Finding nontrivial candidates for provably beneficial self-modifications requires either tremendous foresight or intractable search.</p>
<p>Recently a class of AGI-aspiring systems that we call experience-based AI (EXPAI) has emerged, which fix/circumvent/trivialize these issue. They are self-improving systems that make tentative, additive, reversible, very fine-grained modifications, without prior self-reasoning; instead, self-modifications are tested over time against experiential evidences and slowly phased in when vindicated or dismissed when falsified. We expect EXPAI to have high impact due to its practicality and tractability. Therefore we must now study how EXPAI implementations can be molded and tested during their early growth period to ensure their robust adherence to benevolence constraints.</p>
<p>In this project, the Swiss AI lab IDSIA will deliver an EXPAI growth control methodology that shall be widely applicable.</p>
<h3><a class="adjust_anchor" name="Veloso"></a><a href="http://www.cs.cmu.edu/~mmv/">Manuela Veloso</a></h3>
<p><i>Project Summary: </i>We focus on current and future complex AI autonomous systems that integrate sensors, computation, and actuation to perform tasks of benefit to humans. Examples of such systems are auto-pilots, medical assistants, internet-of-things components, and mobile service robots. One of the key aspects to bring such complex AI systems to safe and acceptable existence is the ability for such systems to provide transparency on their representations, interpretations, choices, and decisions, in summary, their internal state.</p>
<p>We believe that, to build AI systems that are safe, as well as accepted and trusted by humans, we need to equip them with the capability to explain their actions, recommendations, and inferences. Our proposed project aims at researching on the specification, formalization, and generation of explanations, with a concrete focus on seamlessly integrated AI systems that sense and reason about multi-modal information in symbiosis with humans. As a result, humans will be able to query robots for explanations about their recommendations or actions, and carry any needed corrections.</p>
<p><strong>Technical Abstract: </strong>AI systems have long been challenged with providing explanations about their reasoning. Automated theorem provers, explanation-based learning systems, and conflict-based constraint solvers are examples where inference is supplemented by the underlying processed knowledge and rules.</p>
<p>We focus on current and future complex AI autonomous systems that integrate perception, cognition, and action, in tasks to service humans. These systems can be viewed as cyber-physical-social systems, such as auto-pilots, medical assistants, internet-of-things components, and mobile service robots.</p>
<p>We propose to research on bringing such complex AI systems to safe and acceptable existence by providing transparency on their representations, interpretations, choices, and decisions. We will develop mining techniques to enable the analysis and explanation of temporally-logged sensory and execution data, constrained by the underlying behavior architecture, as well as the uncertainty of the sensed environment. We will address the need for probabilistic and knowledge-based inference; the variety of input data modalities; and the coordination of multiple reasoning agents.</p>
<p>We will concretely research on autonomous mobile service robots, such as CoBots, as well as quadrotors. We envision humans setting queries about the robots performance and the choice of their actions. Our generated explanations will increase the understanding, and robot safety.</p>
<h3><a class="adjust_anchor" name="Webb"></a><a href="http://economics.stanford.edu/node/5223">Michael Webb</a></h3>
<p><i>Project Summary: </i>Progress towards a fully-automated economy suffers from a profound tension. On the one hand, technological progress depends on human effort. Human effort is, in general, decreasing in the amount that effort is taxed. On the other hand, the more the economy is automated, the more redistribution could be required to support the living standards of the less skilled. The less skilled could even become unemployed, and the unemployed could eventually comprise the majority of the population. The higher the fraction unemployed, the higher must be the tax burden on those who are productive in this new economy.</p>
<p>At first glance, then, the more technological progress we make, the more we will be forced to disincentivize further progress. Yet, it is possible that some paths of tax and subsidy policy could lead to vastly improved social welfare a few decades hence compared to others. Some paths might avoid altogether the scenario sketched above. This project seeks to characterize the path of optimal policy in the transition to a fully-automated economy. In doing so, it would answer directly the question of how we maximize the societal benefit of AI.</p>
<p><strong>Technical Abstract: </strong>Progress towards a fully-automated economy suffers from a profound tension. On the one hand, technological progress depends on human effort. Human effort is, in general, decreasing in the amount that effort is taxed. On the other hand, the more the economy is automated, the more redistribution could be required to support the living standards of the less skilled. The less skilled could even become unemployed, and the unemployed could eventually comprise the majority of the population. The higher the fraction unemployed, the higher must be the tax burden on those who are productive in this new economy.</p>
<p>At first glance, then, the more technological progress we make, the more we will be forced to disincentivize further progress. Yet, it is possible that some paths of tax and subsidy policy could lead to vastly improved social welfare a few decades hence compared to others. Some paths might avoid altogether the scenario sketched above. This project seeks to characterize the path of optimal policy in the transition to a fully-automated economy. In doing so, it would answer directly the question of how we maximize the societal benefit of AI.</p>
<h3><a class="adjust_anchor" name="Weld"></a><a href="http://www.cs.washington.edu/people/faculty/weld">Daniel Weld</a></h3>
<p><i>Project Summary: </i>AI systems, whether robotic or conversational software agents, use planning algorithms to achieve high-level goals by exhaustively considering all possible sequences of actions. While these methods are increasingly powerful and can even generate seemly creative solutions, they have no understanding of ethics: they don’t understand harm nor can they distinguish between good and bad side effects of their actions. We propose to develop representations and algorithms fill this gap.</p>
<p><strong>Technical Abstract: </strong>Recent advances in probabilistic planning and reinforcement learning have resulted in impressive performance at tasks as varied as mobile robotics, self-driving cars, and playing Atari video games. As these algorithms get deployed in real-world environments, it becomes critical to ensure that their utility-seeking behavior does not result in unintended, harmful side-effects. We need a way to specify a set of agent ethics: social norms that we can trust the agent will not knowingly violate. Developing mechanisms for defining and enforcing such ethical constraints requires innovations ranging from improved vocabulary grounding to more robust planning and reinforcement learning algorithms.</p>
<h3><a class="adjust_anchor" name="Weller"></a><a href="http://www.cs.columbia.edu/~adrian/">Adrian Weller</a></h3>
<p><i>Project Summary: </i>We are unsure about what moral system is best for humans, let alone for potentially super-intelligent machines. It is likely that we shall need to create artificially intelligent agents to provide moral guidance and police issues of appropriate ethical values and best practice, yet this poses significant challenges. Here we propose an initial evaluation of the strengths and weaknesses of one avenue by investigating self-policing intelligent agents. We shall explore two themes: (i) adding a layer of AI agents whose express purpose is to police other AI agents and report unusual or undesirable activity (potentially this might involve setting traps to catch misbehaving agents, and may consider if it is wise to allow policing agents to take corrective action against offending agents); and (ii) analyzing simple models of evolving adaptive agents to see if robust conclusions can be learned. We aim to survey related literature, identify key areas of hope and concern for future investigation, and obtain preliminary results for possible guarantees. The proposal is for a one year term to explore the ideas and build initial models, which will be made publicly available, ideally in journals or at conferences or workshops, with extensions likely if progress is promising.</p>
<p><strong>Technical Abstract: </strong>We are unsure about what moral system is best for humans, let alone for potentially super-intelligent machines. It is likely that we shall need to create artificially intelligent agents to provide moral guidance and police issues of appropriate ethical values and best practice, yet this poses significant challenges. Here we propose an initial evaluation of the strengths and weaknesses of one avenue by investigating self-policing intelligent agents. We shall explore two themes: (i) adding a layer of AI agents whose express purpose is to police other AI agents and report unusual or undesirable activity (potentially this might involve setting traps to catch misbehaving agents, and may consider if it is wise to allow policing agents to take corrective action against offending agents); and (ii) analyzing simple models of evolving adaptive agents to see if robust conclusions can be learned. We aim to survey related literature, identify key areas of hope and concern for future investigation, and obtain preliminary results for possible guarantees. The proposal is for a one year term to explore the ideas and build initial models, which will be made publicly available, ideally in journals or at conferences or workshops, with extensions likely if progress is promising.</p>
<h3><a class="adjust_anchor" name="Wellman"></a><a href="http://ai.eecs.umich.edu/people/wellman/">Michael Wellman</a></h3>
<p><i>Project Summary: </i>The devastation of the 2008 financial crisis remains a fresh memory seven years later, and its effects still reverberate in the global economy. The loss of trillions of dollars in output, and associated tragedy of displacement for millions of people demonstrate in the most vivid way the crucial role of a functional financial system for modern civilization. Unlike physical disasters, financial crises are essentially information events: shocks in the beliefs and expectations of individuals and organizations–about asset values, ability of counterparties to meet obligations, etc.–that nevertheless have real consequences for everyone.</p>
<p>This pivotal and fragile sector also happens to be at the leading edge of autonomous computational (AI) decision making. For large classes of financial assets, trading is dominated by algorithms, or “bots”, operating at speeds well beyond the scale of human reaction times. This regime change is a fait accompli, despite our unresolved debates and generally poor understanding of its implications for fundamental market stability as well as performance and efficiency.</p>
<p>We propose a systematic in-depth study of AI risks to the financial system. Our goals are to identify the main pathways of concern and generate constructive solutions for making financial infrastructure more robust to interaction with AI participants.</p>
<p><strong>Technical Abstract: </strong>The financial system presents a critical sector of our society, at the leading-edge of AI engagement and especially vulnerable to impact from near-term AI advances. Algorithmic and high-frequency trading now dominate financial markets, yet their implications for market stability are poorly understood. In this project we undertake a systematic investigation of how AI traders can impact market stability, and how extreme movements in securities markets in turn can impact the real economy. We develop a general framework for automated trading based on a flexible architecture for arbitrage reasoning. Through agent-based simulation combined with game-theoretic strategy selection, we search for vulnerabilities in financial markets, and characterize the conditions that enable or prevent their exploitation. A new approach to modeling complex networks of financial obligations is applied to the study of contagion between asset-pricing anomalies and panics in the broader financial system. Results from this study will be employed to design market rules, monitoring technologies, and regulation techniques that promote stability in a world of algorithmic traders.</p>
<h3><a class="adjust_anchor" name="Wooldridge"></a><a href="http://www.cs.ox.ac.uk/people/michael.wooldridge/">Michael Wooldridge</a></h3>
<p><i>Project Summary: </i>Codes of ethics play an important role in many sciences. Such codes aim to provide a framework within which researchers can understand and anticipate the possible ethical issues that their research might raise, and to provide guidelines about what is, and is not, regarded as ethical behaviour. In the medical sciences, for example, codes of ethics are fundamentally embedded within the research culture of the discipline, and explicit consideration of ethical issues is a standard expectation when research projects are planned and undertaken. In this project, we aim to start developing a code of ethics for AI research by learning from this interdisciplinary experience and extending its lessons into new areas. The project will bring together three Oxford researchers with expertise in artificial intelligence, philosophy, and applied ethics.</p>
<p><strong>Technical Abstract: </strong>Codes of ethics play an important role in many sciences. Such codes aim to provide a framework within which researchers can understand and anticipate the possible ethical issues that their research might raise, and to provide guidelines about what is, and is not, regarded as ethical behaviour. In the medical sciences, especially, codes of ethics are fundamentally embedded within the research culture, and explicit consideration of ethical issues is a standard expectation when research projects are planned and undertaken. The aim of this project is to develop a solid basis for a code of artificial intelligence (AI) research ethics, learning from the scientific and medical community’s experience with existing ethical codes, and extending its lessons into three important and representative areas where artificial intelligence comes into contact with ethical concerns: AI in medicine and biomedical technology, autonomous vehicles, and automated trading agents. We will also explore whether the design of ethical research codes might usefully anticipate, and potentially ameliorate, the risks of future research into superintelligence. The project brings together three Oxford researchers with highly relevant expertise in artificial intelligence, philosophy, and applied ethics, and will also draw strongly on other research activity within the University of Oxford.</p>
<h3><a class="adjust_anchor" name="Ziebart"></a><a href="http://www.cs.uic.edu/Ziebart">Brian Ziebart</a></h3>
<p><i>Project Summary: </i>“I don’t know” is a safe and appropriate answer that people provide to many posed questions. To appropriately act in a variety of complex tasks, our artificial intelligence systems should incorporate similar levels of uncertainty. Instead, state-of-the-art statistical models and algorithms that enable computer systems to answer such questions based on previous experience often produce overly confident answers. Due to widely used modeling assumptions, this is particularly true when new questions come from situations that differ substantially from previous experience. In other words, exactly when human-level intelligence provides less certainty when generalizing from the known to the unknown, artificial intelligence tends to provide more. Rather than trying to engineer fixes to this phenomenon into existing methods, We propose a more pessimistic approach based on the question: “What is the worst-case possible for predictive data that still matches with previous experiences (observations)?” We propose to analyze the theoretical benefits of this approach and demonstrate its applied benefits on prediction tasks.</p>
<p><strong>Technical Abstract: </strong>Reliable inductive reasoning that uses previous experiences to make predictions of unseen information in new situations is a key requirement for enabling useful artificial intelligence systems.</p>
<p>Tasks ranging over recognizing objects in camera images, predicting the outcomes of possible autonomous system controls, and understanding the intentions of other intelligent entities each depend on this type of reasoning. Unfortunately, existing techniques produce significant unforeseen errors when the underlying statistical assumptions they are based upon do not hold in reality. The nearly ubiquitous assumption that estimated relationships in future situations will be similar to previous experiences (i.e., past and future data is assumed to be exchangeable or independent and identically distributed–IID–according to a common distribution) is particularly brittle when employed within artificial intelligence systems that autonomously interact with the physical world. We propose an adversarial formulation for cost-sensitive prediction under covariate shift—a relaxation of this statistical assumption. This approach provides robustness to data shifts between predictive model estimation and deployment while incorporating mistake-specific costs for different errors that can be tied to application outcomes. We propose theoretical analysis and experimental investigation of this approach for standard and active learning tasks.</p>
<h3><a class="adjust_anchor" name="Bostrom"></a><a href="http://www.nickbostrom.com/">Nick Bostrom</a></h3>
<p><i>Project Summary: </i>We propose the creation of a joint Oxford-Cambridge research center, which will develop policies to be enacted by governments, industry leaders, and others in order to minimize risks and maximize benefit from artificial intelligence (AI) development in the longer term. The center will focus explicitly on the long-term impacts of AI, the strategic implications of powerful AI systems as they come to exceed human capabilities in most domains of interest, and the policy responses that could best be used to mitigate the potential risks of this technology.</p>
<p>There are reasons to believe that unregulated and unconstrained development could incur significant dangers, both from “bad actors” like irresponsible governments, and from the unprecedented capability of the technology itself. For past high-impact technologies (e.g. nuclear fission), policy has often followed implementation, giving rise to catastrophic risks. It is important to avoid this with superintelligence: safety strategies, which may require decades to implement, must be developed before broadly superhuman, general-purpose AI becomes feasible.</p>
<p>This center represents a step change in technology policy: a comprehensive initiative to formulate, analyze, and test policy and regulatory approaches for a transformative technology in advance of its creation.</p>
<p><strong>Technical Abstract: </strong><a href="http://futureoflife.org/static/data/full-proposal-files/Strategic_Research_Center_f_6.pdf?x57718">here</a></p>
<h3><a class="adjust_anchor" name="Salamon"></a><a href="http://rationality.org/about/">Anna Salamon</a></h3>
<p><i>Project Summary: </i>It is crucial for AI researchers to be able to reason carefully about the potential risks of AI, and about how to maximize the odds that any superintelligence that develops remains aligned with human values (in what the Future of Life Institute refers to as the “AI alignment problem”).</p>
<p>Unfortunately, cognitive science research has demonstrated that even very high-IQ humans are subject to many biases that are especially likely to impact their judgment on AI alignment. Leaders in the nascent field of AI alignment have found that a deep familiarity with cognitive bias research, and practice overcoming those biases, has been crucial to progress in the field.</p>
<p>We therefore propose to help spread key reasoning skills and community norms throughout the AI community, via the following:</p>
<ol>
<li>In 2016, we will hold a workshop for 45 of the most promising AI students (graduate, undergraduate, and postdocs), in which we train them in the thinking skills most relevant to AI alignment.</li>
<li>We will maintain contact with AI students after the workshop, helping them to stay in contact with the alignment issue and collaborate with each other to spread useful skills throughout the community and discover new ones themselves.</li>
</ol>
<h3><a class="adjust_anchor" name="Steinhardt"></a><a href="http://cs.stanford.edu/~jsteinhardt/">Jacob Steinhardt</a></h3>
<p><i>Project Summary: </i>The impact of AI on society depends not only on the technical state of AI research, but also its sociological state. Thus, in addition to current AI safety research, we must also ensure that the next generation of AI researchers is composed of thoughtful, intelligent, safety-conscious individuals. The more the AI community as a whole consists of such skilled, broad-minded reasoners, the more likely AI is to be developed in a safe and beneficial manner.</p>
<p>Therefore, we propose running a summer program for extraordinarily gifted high school students (such as competitors from the International Mathematics Olympiad), with an emphasis on artificial intelligence, cognitive debiasing, and choosing a high-positive-impact career path, including AI safety research as a primary consideration. Many of our classes will be about AI and related technical areas, with two classes specifically about the impacts of AI on society.</p>
<h3><a class="adjust_anchor" name="Vardi"></a><a href="http://www.cs.rice.edu/~vardi/">Moshe Vardi</a></h3>
<p><i>Project Summary: </i>We propose to hold a one-day summit (in spring 2017) at Washington, DC, on the subject of artificial intelligence (broadly conceived) and the future of work. The goal is to put this issue on the national agenda in an informed and deliberate manner rather than the typically-alarmist and over-the-top accounts disseminated by the mainstream media. The location is important to ensure attendance by policy makers and leaders of funding agencies. The summit will bring together leading technologists, economists, sociologists, and humanists, who will offer the views on where technology is going, what its impact may be, and what research issues are raised by these projections.</p>
<p>The summit will be sponsored by the Computing Research Association (CRA), whose Government Affairs Committee has extensive experience of reaching out to policy makers. We will also reach out to other relevant societies, such as US-ACM, and AAAS.</p>
<h3><a class="adjust_anchor" name="Wallach"></a><a href="http://www.wendellwallach.com/">Wendell Wallach</a></h3>
<p><i>Project Summary: </i>Driverless cars, service robots, surveillance drones, computer networks collecting data, and autonomous weapons are just a few examples of increasingly intelligent technologies scientists are developing. As they progress, researchers face a series of questions about whether these machines can be designed and engineered to take morally significant actions previously reserved for human actors. Can they ensure that artificially intelligent systems will always be demonstrably beneficial, safe, controllable, and sensitive to human values? Many individuals and groups have begun tackling the various subprojects entailed in this challenge. They are, however, often unaware of efforts in complementary fields. Thus they lose opportunities for creative collaboration, miss gaps in their own research, and reproduce work being performed by potential colleagues. The Hastings Center proposes to convene a series of three solution-directed workshops with national and international experts in the various pertinent fields. Together they will develop collaborative strategies and research projects, and forge an outline for a comprehensive plan to insure autonomous systems will be demonstrably beneficial, and that this innovative research progresses in a responsible manner. The results of the workshop will be conveyed through a special report, a dedicated edition of a scholarly journal, and two public symposia.</p>
<p><strong>Technical Abstract: </strong>The vast array of challenges entailed in designing, engineering, and implementing demonstrably beneficial, safe and controllable AI systems are slowly being addressed by scholars working on distinct research trajectories across many disciplines. They are often unaware of efforts in complementary fields, thus losing opportunities for creative synergies, missing gaps in their own research, and reproducing the work of potential colleagues. The Hastings Center proposes to convene a series of three solution-directed workshops with national and international experts in the varied fields. Together they will address trans-disciplinary questions, develop collaborative strategies and research projects, and forge an outline for a comprehensive plan encompassing the many elements of ensuring autonomous systems will be demonstrably beneficial, and that this innovative research progresses in a responsible manner. The workshops’ research and policy agenda will be published as a Special Report of the journal Hastings Center Report and in short form in a science or engineering journal. Findings will also be presented through two public symposia, one of which will be webcast and available on demand. We anticipate significant progress given the high caliber of the people who are excited by this project and have already committed to join our workshops.</p>
</div>
<div class="clearDiv"></div>
</div></section>
</div></div></main> <aside class='sidebar sidebar_right smartphones_sidebar_active alpha units' role="complementary" itemscope="itemscope" itemtype="https://schema.org/WPSideBar"><div class='inner_sidebar extralight-border'><section id="newsbox-13" class="widget clearfix newsbox"><h3 class="widgettitle">FLI Projects &#038; Updates</h3><ul class="news-wrap image_size_widget"><li class="news-content post-format-standard"><a class='news-link' title='2018 International AI Safety Grants Competition' href='https://futureoflife.org/2017/12/20/2018-international-ai-safety-grants-competition/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2017/12/2018-AI-safety-grants-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>2018 International AI Safety Grants Competition<span class='news-time'>December 20, 2017 - 6:51 am</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='AI Researchers Create Video to Call for Autonomous Weapons Ban at UN' href='https://futureoflife.org/2017/11/14/ai-researchers-create-video-call-autonomous-weapons-ban-un/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2017/11/rendered-slaughterbot-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>AI Researchers Create Video to Call for Autonomous Weapons Ban at UN<span class='news-time'>November 14, 2017 - 12:28 am</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='55 Years After Preventing Nuclear Attack, Arkhipov Honored With Inaugural Future of Life Award' href='https://futureoflife.org/2017/10/27/55-years-preventing-nuclear-attack-arkhipov-honored-inaugural-future-life-award/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2017/10/Arkhipov-young-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>55 Years After Preventing Nuclear Attack, Arkhipov Honored With Inaugural Future of Life Award<span class='news-time'>October 27, 2017 - 2:04 pm</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='An Open Letter to the United Nations Convention on Certain Conventional Weapons' href='https://futureoflife.org/autonomous-weapons-open-letter-2017/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2017/08/02-MQ-9_Reaper-drone1-small-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>An Open Letter to the United Nations Convention on Certain Conventional Weapons<span class='news-time'>August 20, 2017 - 8:01 am</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='Superintelligence survey' href='https://futureoflife.org/superintelligence-survey/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2017/08/forecast-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>Superintelligence survey<span class='news-time'>August 15, 2017 - 2:56 pm</span></strong></a></li></ul><span class="seperator extralight-border"></span></section><section id="avia_combo_widget-3" class="widget clearfix avia_combo_widget"><div class='tabcontainer border_tabs top_tab tab_initial_open tab_initial_open__1'><div class="tab first_tab active_tab widget_tab_popular"><span>Popular</span></div><div class='tab_content active_tab_content'><ul class="news-wrap"><li class="news-content post-format-standard"><a class='news-link' title='A Principled AI Discussion in Asilomar' href='https://futureoflife.org/2017/01/17/principled-ai-discussion-asilomar/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2017/01/principled_conversation-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>A Principled AI Discussion in Asilomar<span class='news-time'>January 17, 2017 - 8:34 pm</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='Elon Musk donates $10M to keep AI beneficial' href='https://futureoflife.org/2015/10/12/elon-musk-donates-10m-to-keep-ai-beneficial/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2015/10/elon-musk-new-e1445639843113-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>Elon Musk donates $10M to keep AI beneficial<span class='news-time'>October 12, 2015 - 12:28 am</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='How Do We Align Artificial Intelligence with Human Values?' href='https://futureoflife.org/2017/02/03/align-artificial-intelligence-with-human-values/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2017/02/value_alignment_hands-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>How Do We Align Artificial Intelligence with Human Valu...<span class='news-time'>February 3, 2017 - 4:58 pm</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='Can We Properly Prepare for the Risks of Superintelligent AI?' href='https://futureoflife.org/2017/03/23/ai-risks-principle/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2017/03/Risks-Principle-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>Can We Properly Prepare for the Risks of Superintelligent...<span class='news-time'>March 23, 2017 - 4:33 pm</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='Preparing for the Biggest Change in Human History' href='https://futureoflife.org/2017/02/24/importance-principle/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2017/02/importance-principle-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>Preparing for the Biggest Change in Human History<span class='news-time'>February 24, 2017 - 4:25 pm</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='The Top A.I. Breakthroughs of 2015' href='https://futureoflife.org/2015/12/29/the-top-a-i-breakthroughs-of-2015/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2015/12/robot_hands_puzzle-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>The Top A.I. Breakthroughs of 2015<span class='news-time'>December 29, 2015 - 5:10 pm</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='When AI Journalism Goes Bad' href='https://futureoflife.org/2016/04/26/ai-journalism-goes-bad/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2016/04/bad_AI_journalism-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="A rebuttal to bad AI journalism" /></span><strong class='news-headline'>When AI Journalism Goes Bad<span class='news-time'>April 26, 2016 - 12:39 pm</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='Think-tank dismisses leading AI researchers as luddites' href='https://futureoflife.org/2015/12/24/think-tank-dismisses-leading-ai-researchers-as-luddites/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2015/10/elon_2x11-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>Think-tank dismisses leading AI researchers as luddites<span class='news-time'>December 24, 2015 - 11:43 am</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='Risks From General Artificial Intelligence Without an Intelligence Explosion' href='https://futureoflife.org/2015/11/30/risks-from-general-artificial-intelligence-without-an-intelligence-explosion/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2015/11/risks_from_general_AI-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>Risks From General Artificial Intelligence Without an Intelligence...<span class='news-time'>November 30, 2015 - 10:30 am</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='How Smart Can AI Get?' href='https://futureoflife.org/2017/02/17/capability-caution-principle/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2017/02/capability-caution-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>How Smart Can AI Get?<span class='news-time'>February 17, 2017 - 3:24 pm</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='Existential Risks Are More Likely to Kill You Than Terrorism' href='https://futureoflife.org/2016/06/29/existential-risks-likely-kill-terrorism/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2016/06/existential-risk-terrorism-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>Existential Risks Are More Likely to Kill You Than Terr...<span class='news-time'>June 29, 2016 - 10:27 am</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='The Superintelligence Control Problem' href='https://futureoflife.org/2015/11/23/the-superintelligence-control-problem/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2015/11/neurons_artificial_intelligence-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>The Superintelligence Control Problem<span class='news-time'>November 23, 2015 - 1:36 pm</span></strong></a></li></ul></div><div class="tab widget_tab_recent"><span>Recent</span></div><div class='tab_content'><ul class="news-wrap"><li class="news-content post-format-standard"><a class='news-link' title='A Principled AI Discussion in Asilomar' href='https://futureoflife.org/2017/01/17/principled-ai-discussion-asilomar/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2017/01/principled_conversation-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>A Principled AI Discussion in Asilomar<span class='news-time'>January 17, 2017 - 8:34 pm</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='Elon Musk donates $10M to keep AI beneficial' href='https://futureoflife.org/2015/10/12/elon-musk-donates-10m-to-keep-ai-beneficial/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2015/10/elon-musk-new-e1445639843113-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>Elon Musk donates $10M to keep AI beneficial<span class='news-time'>October 12, 2015 - 12:28 am</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='As CO2 Levels Rise, Scientists Question Best- and Worst-Case Scenarios of Climate Change' href='https://futureoflife.org/2018/02/06/if-atmospheric-co2-doubles-how-hot-will-it-get/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2018/02/co2-emissions_sunset-2-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>As CO2 Levels Rise, Scientists Question Best- and Worst-Case...<span class='news-time'>February 6, 2018 - 7:00 am</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='FLI January, 2018 Newsletter' href='https://futureoflife.org/2018/02/03/fli-january-2018-newsletter/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2018/02/AI-breakthroughs-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>FLI January, 2018 Newsletter<span class='news-time'>February 3, 2018 - 4:19 pm</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='Podcast: Top AI Breakthroughs and Challenges of 2017 with Richard Mallah and Chelsea Finn' href='https://futureoflife.org/2018/01/31/podcast-top-ai-breakthroughs-and-challenges-of-2017-with-richard-mallah-and-chelsea-finn/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2017/10/alphago-zero-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" srcset="https://futureoflife.org/wp-content/uploads/2017/10/alphago-zero-220x100.jpg 220w, https://futureoflife.org/wp-content/uploads/2017/10/alphago-zero-1382x630.jpg 1382w" sizes="(max-width: 220px) 100vw, 220px" /></span><strong class='news-headline'>Podcast: Top AI Breakthroughs and Challenges of 2017 with...<span class='news-time'>January 31, 2018 - 2:57 pm</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='Is There a Trade-off Between Immediate and Longer-term AI Safety Efforts?' href='https://futureoflife.org/2018/01/29/trade-off-immediate-longer-term-ai-safety-efforts/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2016/08/robot_AI_safety-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>Is There a Trade-off Between Immediate and Longer-term AI...<span class='news-time'>January 29, 2018 - 6:01 pm</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='MIRI&#8217;s January 2018 Newsletter' href='https://futureoflife.org/2018/01/29/miris-january-2018-newsletter/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2015/11/miri_horizontal_1000px-e1447624329777-220x100.png?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>MIRI&#8217;s January 2018 Newsletter<span class='news-time'>January 29, 2018 - 2:24 pm</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='Rewinding the Doomsday Clock' href='https://futureoflife.org/2018/01/26/rewinding-the-doomsday-clock/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2018/01/battleships_north-korea-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>Rewinding the Doomsday Clock<span class='news-time'>January 26, 2018 - 8:45 pm</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='FLI December, 2017 Newsletter' href='https://futureoflife.org/2018/01/18/fli-december-2017-newsletter/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2017/12/2018-AI-safety-grants-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>FLI December, 2017 Newsletter<span class='news-time'>January 18, 2018 - 1:43 am</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='AI Should Provide a Shared Benefit for as Many People as Possible' href='https://futureoflife.org/2018/01/10/shared-benefit-principle/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2018/01/Shared-Benefit-Principle-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>AI Should Provide a Shared Benefit for as Many People as...<span class='news-time'>January 10, 2018 - 11:52 am</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='Deep Safety: NIPS 2017 Report' href='https://futureoflife.org/2018/01/03/deep-safety-nips-2017-report/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2018/01/convention_center__hero-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" /></span><strong class='news-headline'>Deep Safety: NIPS 2017 Report<span class='news-time'>January 3, 2018 - 1:10 pm</span></strong></a></li><li class="news-content post-format-standard"><a class='news-link' title='Training Artificial Intelligence to Compromise Chinese' href='https://futureoflife.org/2018/01/03/training-artificial-intelligence-compromise-chinese/'><span class='news-thumb '><img width="220" height="100" src="https://futureoflife.org/wp-content/uploads/2016/09/Training-AI-to-compromise-845x321-1-220x100.jpg?x57718" class="attachment-widget size-widget wp-post-image" alt="" srcset="https://futureoflife.org/wp-content/uploads/2016/09/Training-AI-to-compromise-845x321-1-220x100.jpg 220w, https://futureoflife.org/wp-content/uploads/2016/09/Training-AI-to-compromise-845x321-1-710x321.jpg 710w" sizes="(max-width: 220px) 100vw, 220px" /></span><strong class='news-headline'>Training Artificial Intelligence to Compromise Chinese<span class='news-time'>January 3, 2018 - 2:16 am</span></strong></a></li></ul></div><div class="tab widget_tab_comments"><span>Comments</span></div><div class='tab_content'><ul class="news-wrap"><li class="news-content"><a class='news-link' title='AI Should Provide a Shared Benefit for as Many People as Possible' href='https://futureoflife.org/2018/01/10/shared-benefit-principle/#comment-6085'><span class='news-thumb'><img alt='Murray' src='https://secure.gravatar.com/avatar/62646a9afc8ac94741e984da6b44023c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/62646a9afc8ac94741e984da6b44023c?s=96&amp;d=mm&amp;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /></span><strong class='news-headline'>As a start shouldn't we ensure the distribution and dissemination...<span class='news-time'>February 2, 2018 - 4:47 pm by Murray</span></strong></a></li><li class="news-content"><a class='news-link' title='Can We Properly Prepare for the Risks of Superintelligent AI?' href='https://futureoflife.org/2017/03/23/ai-risks-principle/#comment-6066'><span class='news-thumb'><img alt='Mehrdad' src='https://secure.gravatar.com/avatar/c0fd4579b290c79a6027152b20e3c622?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/c0fd4579b290c79a6027152b20e3c622?s=96&amp;d=mm&amp;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /></span><strong class='news-headline'>First of all, I don’t agree with the term 'AI' because...<span class='news-time'>January 31, 2018 - 10:02 am by Mehrdad</span></strong></a></li><li class="news-content"><a class='news-link' title='AI Should Provide a Shared Benefit for as Many People as Possible' href='https://futureoflife.org/2018/01/10/shared-benefit-principle/#comment-6001'><span class='news-thumb'><img alt='Koen Buckinx' src='https://secure.gravatar.com/avatar/b5fae6155423b8bebf42dcbf1663d435?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/b5fae6155423b8bebf42dcbf1663d435?s=96&amp;d=mm&amp;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /></span><strong class='news-headline'>Is there an age restriction? Wouldn’t the lagest homogenous...<span class='news-time'>January 10, 2018 - 4:33 pm by Koen Buckinx</span></strong></a></li><li class="news-content"><a class='news-link' title='Research for Beneficial Artificial Intelligence' href='https://futureoflife.org/2017/12/27/research-for-beneficial-artificial-intelligence/#comment-6000'><span class='news-thumb'><img alt='A.T. Murray' src='https://secure.gravatar.com/avatar/f6d85b7feb32d0b81d55ca21352e4b89?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/f6d85b7feb32d0b81d55ca21352e4b89?s=96&amp;d=mm&amp;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /></span><strong class='news-headline'>I've been coding an actual AI with heavy intensity for many...<span class='news-time'>January 10, 2018 - 7:55 am by A.T. Murray</span></strong></a></li><li class="news-content"><a class='news-link' title='Research for Beneficial Artificial Intelligence' href='https://futureoflife.org/2017/12/27/research-for-beneficial-artificial-intelligence/#comment-5998'><span class='news-thumb'><img alt='akeeckerwall' src='https://secure.gravatar.com/avatar/d339082d908bf0330bcad97d3e92e614?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/d339082d908bf0330bcad97d3e92e614?s=96&amp;d=mm&amp;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /></span><strong class='news-headline'>Perhaps a future for Homo Sapiens (HS) in analogy with the...<span class='news-time'>January 9, 2018 - 3:37 am by akeeckerwall</span></strong></a></li><li class="news-content"><a class='news-link' title='How Do We Align Artificial Intelligence with Human Values?' href='https://futureoflife.org/2017/02/03/align-artificial-intelligence-with-human-values/#comment-5993'><span class='news-thumb'><img alt='Vyacheslav Kalmykov' src='https://secure.gravatar.com/avatar/4840fb819fed0088904f731dc43ab0da?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/4840fb819fed0088904f731dc43ab0da?s=96&amp;d=mm&amp;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /></span><strong class='news-headline'>The obstacle to creating a secure AI is the mathematical...<span class='news-time'>January 7, 2018 - 9:56 am by Vyacheslav Kalmykov</span></strong></a></li><li class="news-content"><a class='news-link' title='Preparing for the Biggest Change in Human History' href='https://futureoflife.org/2017/02/24/importance-principle/#comment-5992'><span class='news-thumb'><img alt='Tanaka Yosuke' src='https://secure.gravatar.com/avatar/95011cc4ba3161626e0611f1581620a2?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/95011cc4ba3161626e0611f1581620a2?s=96&amp;d=mm&amp;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /></span><strong class='news-headline'>Even if it develops without malicious AI, there is a possibility...<span class='news-time'>January 7, 2018 - 8:16 am by Tanaka Yosuke</span></strong></a></li><li class="news-content"><a class='news-link' title='How Do We Align Artificial Intelligence with Human Values?' href='https://futureoflife.org/2017/02/03/align-artificial-intelligence-with-human-values/#comment-5991'><span class='news-thumb'><img alt='Vyacheslav Kalmykov' src='https://secure.gravatar.com/avatar/a952ea92e3c5be3b5d4a2af64684141c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/a952ea92e3c5be3b5d4a2af64684141c?s=96&amp;d=mm&amp;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /></span><strong class='news-headline'>People are no less dangerous for each other than future...<span class='news-time'>January 5, 2018 - 7:54 pm by Vyacheslav Kalmykov</span></strong></a></li><li class="news-content"><a class='news-link' title='Preparing for the Biggest Change in Human History' href='https://futureoflife.org/2017/02/24/importance-principle/#comment-5990'><span class='news-thumb'><img alt='Timothy' src='https://secure.gravatar.com/avatar/e02b6c51bf7a23578bba97dd3664ff58?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/e02b6c51bf7a23578bba97dd3664ff58?s=96&amp;d=mm&amp;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /></span><strong class='news-headline'>Why do you say A I will be groundbreaking, if it's not misused?...<span class='news-time'>January 5, 2018 - 12:11 pm by Timothy</span></strong></a></li><li class="news-content"><a class='news-link' title='How Do We Align Artificial Intelligence with Human Values?' href='https://futureoflife.org/2017/02/03/align-artificial-intelligence-with-human-values/#comment-5988'><span class='news-thumb'><img alt='Vyacheslav Kalmykov' src='https://secure.gravatar.com/avatar/a952ea92e3c5be3b5d4a2af64684141c?s=48&#038;d=mm&#038;r=g' srcset='https://secure.gravatar.com/avatar/a952ea92e3c5be3b5d4a2af64684141c?s=96&amp;d=mm&amp;r=g 2x' class='avatar avatar-48 photo' height='48' width='48' /></span><strong class='news-headline'>It is impossible to align people with each other in a situation...<span class='news-time'>January 5, 2018 - 2:54 am by Vyacheslav Kalmykov</span></strong></a></li></ul></div><div class="tab last_tab widget_tab_tags"><span>Tags</span></div><div class='tab_content tagcloud'><a href="https://futureoflife.org/tag/digital-analogues/" class="tag-cloud-link tag-link-50 tag-link-position-1" style="font-size: 12px;">Digital Analogues</a></div></div><span class="seperator extralight-border"></span></section><section id="synved_social_follow-2" class="widget clearfix widget_synved_social_follow"><h3 class="widgettitle">Follow Us</h3><div><a class="synved-social-button synved-social-button-follow synved-social-size-48 synved-social-resolution-single synved-social-provider-facebook nolightbox" data-provider="facebook" target="_blank" rel="nofollow" title="Follow us on Facebook" href="https://www.facebook.com/futureoflifeinstitute/" style="font-size: 0px; width:48px;height:48px;margin:0;margin-bottom:5px;margin-right:5px;"><img alt="Facebook" title="Follow us on Facebook" class="synved-share-image synved-social-image synved-social-image-follow" width="48" height="48" style="display: inline; width:48px;height:48px; margin: 0; padding: 0; border: none; box-shadow: none;" src="https://futureoflife.org/wp-content/plugins/social-media-feather/synved-social/image/social/regular/96x96/facebook.png?x57718" /></a><a class="synved-social-button synved-social-button-follow synved-social-size-48 synved-social-resolution-single synved-social-provider-twitter nolightbox" data-provider="twitter" target="_blank" rel="nofollow" title="Follow us on Twitter" href="https://twitter.com/FLIxrisk" style="font-size: 0px; width:48px;height:48px;margin:0;margin-bottom:5px;"><img alt="twitter" title="Follow us on Twitter" class="synved-share-image synved-social-image synved-social-image-follow" width="48" height="48" style="display: inline; width:48px;height:48px; margin: 0; padding: 0; border: none; box-shadow: none;" src="https://futureoflife.org/wp-content/plugins/social-media-feather/synved-social/image/social/regular/96x96/twitter.png?x57718" /></a></div><span class="seperator extralight-border"></span></section><section id="search-5" class="widget clearfix widget_search">
<form action="https://futureoflife.org/" id="searchform" method="get" class="">
<div>
<input type="submit" value="" id="searchsubmit" class="button avia-font-entypo-fontello" />
<input type="text" id="s" name="s" value="" placeholder='Search' />
</div>
</form><span class="seperator extralight-border"></span></section></div></aside> </div></div> <div class='container_wrap footer_color' id='footer'>
<div class='container'>
<div class='flex_column av_one_fourth  first el_before_av_one_fourth'><section id="search-6" class="widget clearfix widget_search"><h3 class="widgettitle">Search for a topic:</h3>
<form action="https://futureoflife.org/" id="searchform" method="get" class="">
<div>
<input type="submit" value="" id="searchsubmit" class="button avia-font-entypo-fontello" />
<input type="text" id="s" name="s" value="" placeholder='Search' />
</div>
</form><span class="seperator extralight-border"></span></section></div><div class='flex_column av_one_fourth  el_after_av_one_fourth  el_before_av_one_fourth '><section id="categories-5" class="widget clearfix widget_categories"><h3 class="widgettitle">News and Information</h3><label class="screen-reader-text" for="cat">News and Information</label><select name='cat' id='cat' class='postform'>
<option value='-1'>Select Category</option>
<option class="level-0" value="4">AI</option>
<option class="level-0" value="12">AI background</option>
<option class="level-0" value="32">AI FAQs</option>
<option class="level-0" value="52">AI Research</option>
<option class="level-0" value="6">Biotech</option>
<option class="level-0" value="13">Biotech background</option>
<option class="level-0" value="54">Climate background</option>
<option class="level-0" value="29">Conferences and workshops</option>
<option class="level-0" value="7">Environment</option>
<option class="level-0" value="16">Environment background</option>
<option class="level-0" value="17">Events</option>
<option class="level-0" value="51">Existential Risk</option>
<option class="level-0" value="11">Featured</option>
<option class="level-0" value="10">FLI projects</option>
<option class="level-0" value="30">Grants Program</option>
<option class="level-0" value="25">Newsletters</option>
<option class="level-0" value="28">Newsletters2</option>
<option class="level-0" value="5">Nuclear</option>
<option class="level-0" value="14">Nuclear background</option>
<option class="level-0" value="31">Open Letters</option>
<option class="level-0" value="15">Partner org background</option>
<option class="level-0" value="9">Partner Orgs</option>
<option class="level-0" value="27">Past Events</option>
<option class="level-0" value="55">principles</option>
<option class="level-0" value="56">principles interviews</option>
<option class="level-0" value="19">recent news</option>
<option class="level-0" value="33">Research Priorities</option>
<option class="level-0" value="1">Uncategorized</option>
</select>
<script type='text/javascript'>
/* <![CDATA[ */
(function() {
	var dropdown = document.getElementById( "cat" );
	function onCatChange() {
		if ( dropdown.options[ dropdown.selectedIndex ].value > 0 ) {
			location.href = "https://futureoflife.org/?cat=" + dropdown.options[ dropdown.selectedIndex ].value;
		}
	}
	dropdown.onchange = onCatChange;
})();
/* ]]> */
</script>
<span class="seperator extralight-border"></span></section></div><div class='flex_column av_one_fourth  el_after_av_one_fourth  el_before_av_one_fourth '><section id="mc4wp_form_widget-2" class="widget clearfix widget_mc4wp_form_widget"><h3 class="widgettitle">Newsletter</h3><script type="text/javascript">(function() {
	if (!window.mc4wp) {
		window.mc4wp = {
			listeners: [],
			forms    : {
				on: function (event, callback) {
					window.mc4wp.listeners.push({
						event   : event,
						callback: callback
					});
				}
			}
		}
	}
})();
</script><form id="mc4wp-form-1" class="mc4wp-form mc4wp-form-1530 mc4wp-form-basic" method="post" data-id="1530" data-name="Default sign-up form"><div class="mc4wp-form-fields"><p>
<label>First Name:</label>
<input type="text" name="FNAME" placeholder="Your first name">
</p><p>
<label>Email address: </label>
<input type="email" name="EMAIL" placeholder="Your email address" required />
</p>
<p>
<input type="submit" value="Sign up" />
</p><label style="display: none !important;">Leave this field empty if you're human: <input type="text" name="_mc4wp_honeypot" value="" tabindex="-1" autocomplete="off" /></label><input type="hidden" name="_mc4wp_timestamp" value="1518282000" /><input type="hidden" name="_mc4wp_form_id" value="1530" /><input type="hidden" name="_mc4wp_form_element_id" value="mc4wp-form-1" /></div><div class="mc4wp-response"></div></form><span class="seperator extralight-border"></span></section></div><div class='flex_column av_one_fourth  el_after_av_one_fourth  el_before_av_one_fourth '><section id="text-2" class="widget clearfix widget_text"> <div class="textwidget">Technology is giving life the potential to flourish like never before... Or to self destruct.
Let's make a difference!</div>
<span class="seperator extralight-border"></span></section></div>
</div>

</div>
<footer class='container_wrap socket_color' id='socket' role="contentinfo" itemscope="itemscope" itemtype="https://schema.org/WPFooter">
<div class='container'>
<span class='copyright'>© Copyright - FLI - Future of Life Institute</span>
<ul class='noLightbox social_bookmarks icon_count_2'><li class='social_bookmarks_twitter av-social-link-twitter social_icon_1'><a target='_blank' href='http://twitter.com/FLIxrisk' aria-hidden='true' data-av_icon='' data-av_iconfont='entypo-fontello' title='Twitter'><span class='avia_hidden_link_text'>Twitter</span></a></li><li class='social_bookmarks_facebook av-social-link-facebook social_icon_2'><a target='_blank' href='https://www.facebook.com/futureoflifeinstitute' aria-hidden='true' data-av_icon='' data-av_iconfont='entypo-fontello' title='Facebook'><span class='avia_hidden_link_text'>Facebook</span></a></li></ul>
</div>

</footer>

</div>
</div>
<script type='text/javascript'>function ctSetCookie(c_name, value, def_value){document.cookie = c_name + '=' + escape(value) + '; path=/';}ctSetCookie('ct_checkjs', '1920818358', '0');</script> <script>
		( function ( body ) {
			'use strict';
			body.className = body.className.replace( /\btribe-no-js\b/, 'tribe-js' );
		} )( document.body );
		</script>
<script type='text/javascript'>
 /* <![CDATA[ */  
var avia_framework_globals = avia_framework_globals || {};
    avia_framework_globals.frameworkUrl = 'https://futureoflife.org/wp-content/themes/enfold-3/framework/';
    avia_framework_globals.installedAt = 'https://futureoflife.org/wp-content/themes/enfold-3/';
    avia_framework_globals.ajaxurl = 'https://futureoflife.org/wp-admin/admin-ajax.php';
/* ]]> */ 
</script>
<script type='text/javascript'>
 /* <![CDATA[ */  
var avia_framework_globals = avia_framework_globals || {};
	avia_framework_globals.gmap_api = 'AIzaSyCYVnLh0Uh7cf3FTUJmiDcsuairJ7VVUcQ';
/* ]]> */ 
</script>
<script type='text/javascript'> /* <![CDATA[ */var tribe_l10n_datatables = {"aria":{"sort_ascending":": activate to sort column ascending","sort_descending":": activate to sort column descending"},"length_menu":"Show _MENU_ entries","empty_table":"No data available in table","info":"Showing _START_ to _END_ of _TOTAL_ entries","info_empty":"Showing 0 to 0 of 0 entries","info_filtered":"(filtered from _MAX_ total entries)","zero_records":"No matching records found","search":"Search:","all_selected_text":"All items on this page were selected. ","select_all_link":"Select all pages","clear_selection":"Clear Selection.","pagination":{"all":"All","next":"Next","previous":"Previous"},"select":{"rows":{"0":"","_":": Selected %d rows","1":": Selected 1 row"}},"datepicker":{"dayNames":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"dayNamesShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"dayNamesMin":["S","M","T","W","T","F","S"],"monthNames":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthNamesShort":["January","February","March","April","May","June","July","August","September","October","November","December"],"nextText":"Next","prevText":"Prev","currentText":"Today","closeText":"Done"}};/* ]]> */ </script><script type="text/javascript">(function() {function addEventListener(element,event,handler) {
	if(element.addEventListener) {
		element.addEventListener(event,handler, false);
	} else if(element.attachEvent){
		element.attachEvent('on'+event,handler);
	}
}function maybePrefixUrlField() {
	if(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {
		this.value = "http://" + this.value;
	}
}

var urlFields = document.querySelectorAll('.mc4wp-form input[type="url"]');
if( urlFields && urlFields.length > 0 ) {
	for( var j=0; j < urlFields.length; j++ ) {
		addEventListener(urlFields[j],'blur',maybePrefixUrlField);
	}
}/* test if browser supports date fields */
var testInput = document.createElement('input');
testInput.setAttribute('type', 'date');
if( testInput.type !== 'date') {

	/* add placeholder & pattern to all date fields */
	var dateFields = document.querySelectorAll('.mc4wp-form input[type="date"]');
	for(var i=0; i<dateFields.length; i++) {
		if(!dateFields[i].placeholder) {
			dateFields[i].placeholder = 'YYYY-MM-DD';
		}
		if(!dateFields[i].pattern) {
			dateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';
		}
	}
}

})();</script><script type='text/javascript'>
/* <![CDATA[ */
var ctNocache = {"ajaxurl":"https:\/\/futureoflife.org\/wp-admin\/admin-ajax.php","info_flag":"","set_cookies_flag":"1","blog_home":"https:\/\/futureoflife.org\/"};
/* ]]> */
</script>
<script type='text/javascript' src='https://futureoflife.org/wp-content/plugins/cleantalk-spam-protect/js/apbct-public.js?x57718'></script>
<script type='text/javascript' src='https://futureoflife.org/wp-content/themes/enfold-3/js/avia.js?x57718'></script>
<script type='text/javascript' src='https://futureoflife.org/wp-content/themes/enfold-3/js/shortcodes.js?x57718'></script>
<script type='text/javascript' src='https://futureoflife.org/wp-content/themes/enfold-3/js/aviapopup/jquery.magnific-popup.min.js?x57718'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var mejsL10n = {"language":"en-US","strings":{"Close":"Close","Fullscreen":"Fullscreen","Turn off Fullscreen":"Turn off Fullscreen","Go Fullscreen":"Go Fullscreen","Download File":"Download File","Download Video":"Download Video","Play":"Play","Pause":"Pause","Captions\/Subtitles":"Captions\/Subtitles","None":"None","Time Slider":"Time Slider","Skip back %1 seconds":"Skip back %1 seconds","Video Player":"Video Player","Audio Player":"Audio Player","Volume Slider":"Volume Slider","Mute Toggle":"Mute Toggle","Unmute":"Unmute","Mute":"Mute","Use Up\/Down Arrow keys to increase or decrease volume.":"Use Up\/Down Arrow keys to increase or decrease volume.","Use Left\/Right Arrow keys to advance one second, Up\/Down arrows to advance ten seconds.":"Use Left\/Right Arrow keys to advance one second, Up\/Down arrows to advance ten seconds."}};
var _wpmejsSettings = {"pluginPath":"\/wp-includes\/js\/mediaelement\/"};
/* ]]> */
</script>
<script type='text/javascript' src='https://futureoflife.org/wp-includes/js/mediaelement/mediaelement-and-player.min.js?x57718'></script>
<script type='text/javascript' src='https://futureoflife.org/wp-includes/js/mediaelement/wp-mediaelement.min.js?x57718'></script>
<script type='text/javascript' src='https://futureoflife.org/wp-includes/js/comment-reply.min.js?x57718'></script>
<script type='text/javascript' src='https://futureoflife.org/wp-includes/js/wp-embed.min.js?x57718'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var mc4wp_forms_config = [];
/* ]]> */
</script>
<script type='text/javascript' src='https://futureoflife.org/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?x57718'></script>
<!--[if lte IE 9]>
<script type='text/javascript' src='https://futureoflife.org/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?x57718'></script>
<![endif]-->
<script>jQuery(function() {jQuery('#principles_signup_form').contents().find("head").append(jQuery("<style type='text/css'> .ss-form-heading{display:none;} .ss-top-of-page{padding-top:0;margin-bottom:0;}</style>"));}); </script>
<a href='#top' title='Scroll to top' id='scroll-top-link' aria-hidden='true' data-av_icon='' data-av_iconfont='entypo-fontello'><span class="avia_hidden_link_text">Scroll to top</span></a>
<div id="fb-root"></div>
</body>
</html>
