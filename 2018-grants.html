<!DOCTYPE html>
<html lang="en-US" prefix="og: http://ogp.me/ns#" class=" html_stretched responsive av-preloader-disabled av-default-lightbox  html_header_top html_logo_left html_bottom_nav_header html_menu_left html_large html_header_sticky html_header_shrinking html_header_topbar_active html_mobile_menu_phone html_disabled html_header_searchicon html_content_align_center html_header_unstick_top_disabled html_header_stretch_disabled html_minimal_header html_entry_id_17345 ">
<head>
<meta charset="UTF-8" />

<title>2018 AI Grant Recipients - Future of Life Institute</title>

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<script type='text/javascript'>function ctSetCookie(c_name, value, def_value){document.cookie = c_name + '=' + escape(value) + '; path=/';}ctSetCookie('ct_checkjs', '1985459120', '0');</script>

<link rel="canonical" href="https://futureoflife.org/2018-ai-grant-recipients/" />
<meta property="og:locale" content="en_US" />
<meta property="og:type" content="article" />
<meta property="og:title" content="2018 AI Grant Recipients - Future of Life Institute" />
<meta property="og:url" content="https://futureoflife.org/2018-ai-grant-recipients/" />
<meta property="og:site_name" content="Future of Life Institute" />
<meta property="article:publisher" content="https://www.facebook.com/futureoflifeinstitute" />
<meta property="fb:app_id" content="852322561461934" />
<meta property="og:image" content="https://futureoflife.org/wp-content/uploads/2016/04/FLI_logo_square_250.png" />
<meta property="og:image:secure_url" content="https://futureoflife.org/wp-content/uploads/2016/04/FLI_logo_square_250.png" />
<meta property="og:image:width" content="250" />
<meta property="og:image:height" content="250" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="2018 AI Grant Recipients - Future of Life Institute" />
<meta name="twitter:site" content="@FLIxrisk" />
<meta name="twitter:image" content="http://futureoflife.org/wp-content/uploads/2016/04/FLI_logo_square_250.png" />
<meta name="twitter:creator" content="@FLIxrisk" />

<link rel='dns-prefetch' href='//api.tiles.mapbox.com' />
<link rel='dns-prefetch' href='//ajax.googleapis.com' />
<link rel='dns-prefetch' href='//s.w.org' />
<link rel="alternate" type="application/rss+xml" title="Future of Life Institute &raquo; Feed" href="https://futureoflife.org/feed/" />
<link rel="alternate" type="application/rss+xml" title="Future of Life Institute &raquo; Comments Feed" href="https://futureoflife.org/comments/feed/" />
<link rel="alternate" type="text/calendar" title="Future of Life Institute &raquo; iCal Feed" href="https://futureoflife.org/events/?ical=1" />

<link rel='stylesheet' id='avia-google-webfont' href='//fonts.googleapis.com/css?family=Open+Sans:400,600' type='text/css' media='all' />
<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/11\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/11\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/futureoflife.org\/wp-includes\/js\/wp-emoji-release.min.js?ver=ad912df2fde3058c90093c3891b0bbe6"}};
			!function(a,b,c){function d(a,b){var c=String.fromCharCode;l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,a),0,0);var d=k.toDataURL();l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,b),0,0);var e=k.toDataURL();return d===e}function e(a){var b;if(!l||!l.fillText)return!1;switch(l.textBaseline="top",l.font="600 32px Arial",a){case"flag":return!(b=d([55356,56826,55356,56819],[55356,56826,8203,55356,56819]))&&(b=d([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]),!b);case"emoji":return b=d([55358,56760,9792,65039],[55358,56760,8203,9792,65039]),!b}return!1}function f(a){var c=b.createElement("script");c.src=a,c.defer=c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var g,h,i,j,k=b.createElement("canvas"),l=k.getContext&&k.getContext("2d");for(j=Array("flag","emoji"),c.supports={everything:!0,everythingExceptFlag:!0},i=0;i<j.length;i++)c.supports[j[i]]=e(j[i]),c.supports.everything=c.supports.everything&&c.supports[j[i]],"flag"!==j[i]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[j[i]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(h=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",h,!1),a.addEventListener("load",h,!1)):(a.attachEvent("onload",h),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),g=c.source||{},g.concatemoji?f(g.concatemoji):g.wpemoji&&g.twemoji&&(f(g.twemoji),f(g.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
<link rel='stylesheet' id='mapboxadvcss-css' href='https://futureoflife.org/wp-content/plugins/mapbox-for-wp-advanced/assets/css/mapboxadv-min.css?x40372' type='text/css' media='all' />
<link rel='stylesheet' id='mapboxcss-css' href='https://api.tiles.mapbox.com/mapbox.js/v2.2.2/mapbox.css?ver=ad912df2fde3058c90093c3891b0bbe6' type='text/css' media='all' />
<link rel='stylesheet' id='cookie-notice-front-css' href='https://futureoflife.org/wp-content/plugins/cookie-notice/css/front.min.css?x40372' type='text/css' media='all' />
<link rel='stylesheet' id='h5p-plugin-styles-css' href='https://futureoflife.org/wp-content/plugins/h5p/h5p-php-library/styles/h5p.css?x40372' type='text/css' media='all' />
<link rel='stylesheet' id='avia-grid-css' href='https://futureoflife.org/wp-content/themes/enfold-4/css/grid.css?x40372' type='text/css' media='all' />
<link rel='stylesheet' id='avia-base-css' href='https://futureoflife.org/wp-content/themes/enfold-4/css/base.css?x40372' type='text/css' media='all' />
<link rel='stylesheet' id='avia-layout-css' href='https://futureoflife.org/wp-content/themes/enfold-4/css/layout.css?x40372' type='text/css' media='all' />
<link rel='stylesheet' id='avia-scs-css' href='https://futureoflife.org/wp-content/themes/enfold-4/css/shortcodes.css?x40372' type='text/css' media='all' />
<link rel='stylesheet' id='avia-popup-css-css' href='https://futureoflife.org/wp-content/themes/enfold-4/js/aviapopup/magnific-popup.css?x40372' type='text/css' media='screen' />
<link rel='stylesheet' id='avia-media-css' href='https://futureoflife.org/wp-content/themes/enfold-4/js/mediaelement/skin-1/mediaelementplayer.css?x40372' type='text/css' media='screen' />
<link rel='stylesheet' id='avia-print-css' href='https://futureoflife.org/wp-content/themes/enfold-4/css/print.css?x40372' type='text/css' media='print' />
<link rel='stylesheet' id='avia-dynamic-css' href='https://futureoflife.org/wp-content/uploads/dynamic_avia/enfold.css?x40372' type='text/css' media='all' />
<link rel='stylesheet' id='avia-custom-css' href='https://futureoflife.org/wp-content/themes/enfold-4/css/custom.css?x40372' type='text/css' media='all' />
<link rel='stylesheet' id='mc4wp-form-basic-css' href='https://futureoflife.org/wp-content/plugins/mailchimp-for-wp/assets/css/form-basic.min.css?x40372' type='text/css' media='all' />
<link rel='stylesheet' id='avia-events-cal-css' href='https://futureoflife.org/wp-content/themes/enfold-4/config-events-calendar/event-mod.css?x40372' type='text/css' media='all' />
<script type='text/javascript' src='https://api.tiles.mapbox.com/mapbox.js/v2.2.2/mapbox.js?ver=1.3.2'></script>
<script type='text/javascript' src='https://futureoflife.org/wp-content/plugins/cleantalk-spam-protect/js/apbct-public.js?x40372'></script>
<script type='text/javascript' src='https://futureoflife.org/wp-includes/js/jquery/jquery.js?x40372'></script>
<script type='text/javascript' src='https://futureoflife.org/wp-includes/js/jquery/jquery-migrate.min.js?x40372'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var ctPublic = {"gdpr_forms":[],"gdpr_text":"By using this form you agree with the storage and processing of your data by using the Privacy Policy on this website."};
/* ]]> */
</script>
<script type='text/javascript' src='https://futureoflife.org/wp-content/plugins/cleantalk-spam-protect/js/apbct-public--gdpr.js?x40372'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var ctNocache = {"ajaxurl":"https:\/\/futureoflife.org\/wp-admin\/admin-ajax.php","info_flag":"","set_cookies_flag":"","blog_home":"https:\/\/futureoflife.org\/"};
/* ]]> */
</script>
<script type='text/javascript' src='https://futureoflife.org/wp-content/plugins/cleantalk-spam-protect/inc/cleantalk_nocache.js?x40372'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var cnArgs = {"ajaxurl":"https:\/\/futureoflife.org\/wp-admin\/admin-ajax.php","hideEffect":"fade","onScroll":"yes","onScrollOffset":"100","cookieName":"cookie_notice_accepted","cookieValue":"true","cookieTime":"7862400","cookiePath":"\/","cookieDomain":"","redirection":"1","cache":"1","refuse":"yes","revoke_cookies":"0","revoke_cookies_opt":"automatic","secure":"1"};
/* ]]> */
</script>
<script type='text/javascript' src='https://futureoflife.org/wp-content/plugins/cookie-notice/js/front.min.js?x40372'></script>
<script type='text/javascript' src='https://futureoflife.org/wp-content/themes/enfold-4/js/avia-compat.js?x40372'></script>
<script type='text/javascript'>
var mejsL10n = {"language":"en","strings":{"mejs.install-flash":"You are using a browser that does not have Flash player enabled or installed. Please turn on your Flash player plugin or download the latest version from https:\/\/get.adobe.com\/flashplayer\/","mejs.fullscreen-off":"Turn off Fullscreen","mejs.fullscreen-on":"Go Fullscreen","mejs.download-video":"Download Video","mejs.fullscreen":"Fullscreen","mejs.time-jump-forward":["Jump forward 1 second","Jump forward %1 seconds"],"mejs.loop":"Toggle Loop","mejs.play":"Play","mejs.pause":"Pause","mejs.close":"Close","mejs.time-slider":"Time Slider","mejs.time-help-text":"Use Left\/Right Arrow keys to advance one second, Up\/Down arrows to advance ten seconds.","mejs.time-skip-back":["Skip back 1 second","Skip back %1 seconds"],"mejs.captions-subtitles":"Captions\/Subtitles","mejs.captions-chapters":"Chapters","mejs.none":"None","mejs.mute-toggle":"Mute Toggle","mejs.volume-help-text":"Use Up\/Down Arrow keys to increase or decrease volume.","mejs.unmute":"Unmute","mejs.mute":"Mute","mejs.volume-slider":"Volume Slider","mejs.video-player":"Video Player","mejs.audio-player":"Audio Player","mejs.ad-skip":"Skip ad","mejs.ad-skip-info":["Skip in 1 second","Skip in %1 seconds"],"mejs.source-chooser":"Source Chooser","mejs.stop":"Stop","mejs.speed-rate":"Speed Rate","mejs.live-broadcast":"Live Broadcast","mejs.afrikaans":"Afrikaans","mejs.albanian":"Albanian","mejs.arabic":"Arabic","mejs.belarusian":"Belarusian","mejs.bulgarian":"Bulgarian","mejs.catalan":"Catalan","mejs.chinese":"Chinese","mejs.chinese-simplified":"Chinese (Simplified)","mejs.chinese-traditional":"Chinese (Traditional)","mejs.croatian":"Croatian","mejs.czech":"Czech","mejs.danish":"Danish","mejs.dutch":"Dutch","mejs.english":"English","mejs.estonian":"Estonian","mejs.filipino":"Filipino","mejs.finnish":"Finnish","mejs.french":"French","mejs.galician":"Galician","mejs.german":"German","mejs.greek":"Greek","mejs.haitian-creole":"Haitian Creole","mejs.hebrew":"Hebrew","mejs.hindi":"Hindi","mejs.hungarian":"Hungarian","mejs.icelandic":"Icelandic","mejs.indonesian":"Indonesian","mejs.irish":"Irish","mejs.italian":"Italian","mejs.japanese":"Japanese","mejs.korean":"Korean","mejs.latvian":"Latvian","mejs.lithuanian":"Lithuanian","mejs.macedonian":"Macedonian","mejs.malay":"Malay","mejs.maltese":"Maltese","mejs.norwegian":"Norwegian","mejs.persian":"Persian","mejs.polish":"Polish","mejs.portuguese":"Portuguese","mejs.romanian":"Romanian","mejs.russian":"Russian","mejs.serbian":"Serbian","mejs.slovak":"Slovak","mejs.slovenian":"Slovenian","mejs.spanish":"Spanish","mejs.swahili":"Swahili","mejs.swedish":"Swedish","mejs.tagalog":"Tagalog","mejs.thai":"Thai","mejs.turkish":"Turkish","mejs.ukrainian":"Ukrainian","mejs.vietnamese":"Vietnamese","mejs.welsh":"Welsh","mejs.yiddish":"Yiddish"}};
</script>
<script type='text/javascript' src='https://futureoflife.org/wp-includes/js/mediaelement/mediaelement-and-player.min.js?x40372'></script>
<script type='text/javascript' src='https://futureoflife.org/wp-includes/js/mediaelement/mediaelement-migrate.min.js?x40372'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var _wpmejsSettings = {"pluginPath":"\/wp-includes\/js\/mediaelement\/","classPrefix":"mejs-","stretching":"responsive"};
/* ]]> */
</script>
<link rel='https://api.w.org/' href='https://futureoflife.org/wp-json/' />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://futureoflife.org/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://futureoflife.org/wp-includes/wlwmanifest.xml" />
<link rel='shortlink' href='https://futureoflife.org/?p=17345' />
<link rel="alternate" type="application/json+oembed" href="https://futureoflife.org/wp-json/oembed/1.0/embed?url=https%3A%2F%2Ffutureoflife.org%2F2018-ai-grant-recipients%2F" />
<link rel="alternate" type="text/xml+oembed" href="https://futureoflife.org/wp-json/oembed/1.0/embed?url=https%3A%2F%2Ffutureoflife.org%2F2018-ai-grant-recipients%2F&#038;format=xml" />
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ 
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), 
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) 
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); 
ga('create', 'UA-60471514-1', 'futureoflife.org'); 
ga('set', 'anonymizeIp', true); 
ga('send', 'pageview');
</script><meta name="tec-api-version" content="v1"><meta name="tec-api-origin" content="https://futureoflife.org"><link rel="https://theeventscalendar.com/" href="https://futureoflife.org/wp-json/tribe/events/v1/" /><link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="alternate" type="application/rss+xml" title="Future of Life Institute RSS2 Feed" href="https://futureoflife.org/feed/" />
<link rel="pingback" href="https://futureoflife.org/xmlrpc.php" />
<!--[if lt IE 9]><script src="https://futureoflife.org/wp-content/themes/enfold-4/js/html5shiv.js?x40372"></script><![endif]--><style type="text/css">/* MailChimp for WP - Checkbox Styles */
.mc4wp-checkbox-wp-registration-form {
  clear: both;
  display: block;
  position: static;
  width: auto; }
  .mc4wp-checkbox-wp-registration-form input {
    float: none;
    width: auto;
    position: static;
    margin: 0 6px 0 0;
    padding: 0;
    vertical-align: middle;
    display: inline-block !important;
    max-width: 21px;
    -webkit-appearance: checkbox; }
  .mc4wp-checkbox-wp-registration-form label {
    float: none;
    display: block;
    cursor: pointer;
    width: auto;
    position: static;
    margin: 0 0 16px 0; }
</style>
<style type="text/css"></style>
<style type="text/css">
.synved-social-resolution-single {
display: inline-block;
}
.synved-social-resolution-normal {
display: inline-block;
}
.synved-social-resolution-hidef {
display: none;
}

@media only screen and (min--moz-device-pixel-ratio: 2),
only screen and (-o-min-device-pixel-ratio: 2/1),
only screen and (-webkit-min-device-pixel-ratio: 2),
only screen and (min-device-pixel-ratio: 2),
only screen and (min-resolution: 2dppx),
only screen and (min-resolution: 192dpi) {
	.synved-social-resolution-normal {
	display: none;
	}
	.synved-social-resolution-hidef {
	display: inline-block;
	}
}
</style>

<style type='text/css'>
@font-face {font-family: 'entypo-fontello'; font-weight: normal; font-style: normal;
src: url('https://futureoflife.org/wp-content/themes/enfold-4/config-templatebuilder/avia-template-builder/assets/fonts/entypo-fontello.eot?v=3');
src: url('https://futureoflife.org/wp-content/themes/enfold-4/config-templatebuilder/avia-template-builder/assets/fonts/entypo-fontello.eot?v=3#iefix') format('embedded-opentype'), 
url('https://futureoflife.org/wp-content/themes/enfold-4/config-templatebuilder/avia-template-builder/assets/fonts/entypo-fontello.woff?v=3') format('woff'), 
url('https://futureoflife.org/wp-content/themes/enfold-4/config-templatebuilder/avia-template-builder/assets/fonts/entypo-fontello.ttf?v=3') format('truetype'), 
url('https://futureoflife.org/wp-content/themes/enfold-4/config-templatebuilder/avia-template-builder/assets/fonts/entypo-fontello.svg?v=3#entypo-fontello') format('svg');
} #top .avia-font-entypo-fontello, body .avia-font-entypo-fontello, html body [data-av_iconfont='entypo-fontello']:before{ font-family: 'entypo-fontello'; }
</style>
</head>
<body id="top" class="page-template-default page page-id-17345 stretched open_sans cookies-set cookies-accepted tribe-no-js" itemscope="itemscope" itemtype="https://schema.org/WebPage">
<div id='wrap_all'>
<header id='header' class=' header_color dark_bg_color  av_header_top av_logo_left av_bottom_nav_header av_menu_left av_large av_header_sticky av_header_shrinking av_header_stretch_disabled av_mobile_menu_phone av_header_searchicon av_header_unstick_top_disabled av_minimal_header av_header_border_disabled' role="banner" itemscope="itemscope" itemtype="https://schema.org/WPHeader">
<a id="advanced_menu_toggle" href="#" aria-hidden='true' data-av_icon='' data-av_iconfont='entypo-fontello'></a><a id="advanced_menu_hide" href="#" aria-hidden='true' data-av_icon='' data-av_iconfont='entypo-fontello'></a> <div id='header_meta' class='container_wrap container_wrap_meta  av_icon_active_right av_extra_header_active av_secondary_left av_entry_id_17345'>
<div class='container'>
<ul class='noLightbox social_bookmarks icon_count_2'><li class='social_bookmarks_twitter av-social-link-twitter social_icon_1'><a target='_blank' href='http://twitter.com/FLIxrisk' aria-hidden='true' data-av_icon='' data-av_iconfont='entypo-fontello' title='Twitter'><span class='avia_hidden_link_text'>Twitter</span></a></li><li class='social_bookmarks_facebook av-social-link-facebook social_icon_2'><a target='_blank' href='https://www.facebook.com/futureoflifeinstitute' aria-hidden='true' data-av_icon='' data-av_iconfont='entypo-fontello' title='Facebook'><span class='avia_hidden_link_text'>Facebook</span></a></li></ul><nav class='sub_menu' role="navigation" itemscope="itemscope" itemtype="https://schema.org/SiteNavigationElement"><ul id="avia2-menu" class="menu"><li id="menu-item-4" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-4"><a href="http://futureoflife.org/">Home</a></li>
<li id="menu-item-93" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-93"><a href="https://futureoflife.org/team/">Who We Are</a>
<ul class="sub-menu">
<li id="menu-item-10891" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-10891"><a href="https://futureoflife.org/team/">Team</a></li>
<li id="menu-item-2423" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-2423"><a href="http://futureoflife.org/wp-content/uploads/2016/02/FLI-2015-Annual-Report.pdf?x40372">2015 Annual Report</a></li>
<li id="menu-item-13129" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-13129"><a href="https://futureoflife.org/wp-content/uploads/2017/05/FLI-2016-Annual-Report.pdf?x40372">2016 Annual Report</a></li>
<li id="menu-item-15576" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-15576"><a href="http://futureoflife.org/wp-content/uploads/2018/02/FLI-2017-Annual-Report.pdf?x40372">2017 Annual Report</a></li>
<li id="menu-item-10994" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-10994"><a href="https://futureoflife.org/tax-forms/">Tax Forms</a></li>
</ul>
</li>
<li id="menu-item-819" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-819"><a href="https://futureoflife.org/activities-2/">Activities</a>
<ul class="sub-menu">
<li id="menu-item-15962" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-15962"><a href="https://futureoflife.org/activities-2/">Latest Activities</a></li>
<li id="menu-item-16002" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-16002"><a href="https://futureoflife.org/highlights/">Past Highlights</a></li>
<li id="menu-item-815" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-815"><a href="https://futureoflife.org/ai-activities/">AI</a></li>
<li id="menu-item-84" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-84"><a href="https://futureoflife.org/press/">Press</a></li>
<li id="menu-item-2029" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2029"><a href="https://futureoflife.org/newsletters/">Newsletters</a></li>
</ul>
</li>
<li id="menu-item-1427" class="menu-item menu-item-type-post_type menu-item-object-portfolio menu-item-has-children menu-item-1427"><a href="https://futureoflife.org/background/existential-risk/">Existential Risk</a>
<ul class="sub-menu">
<li id="menu-item-15963" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-15963"><a href="https://futureoflife.org/background/existential-risk/">X-risk Overview</a></li>
<li id="menu-item-1430" class="menu-item menu-item-type-post_type menu-item-object-portfolio menu-item-1430"><a href="https://futureoflife.org/background/benefits-risks-of-artificial-intelligence/">Artificial Intelligence</a></li>
<li id="menu-item-1428" class="menu-item menu-item-type-post_type menu-item-object-portfolio menu-item-1428"><a href="https://futureoflife.org/background/the-risk-of-nuclear-weapons/">Nuclear Weapons</a></li>
<li id="menu-item-18558" class="menu-item menu-item-type-post_type menu-item-object-portfolio menu-item-18558"><a href="https://futureoflife.org/background/benefits-risks-biotechnology/">Biotechnology</a></li>
<li id="menu-item-1429" class="menu-item menu-item-type-post_type menu-item-object-portfolio menu-item-1429"><a href="https://futureoflife.org/background/climate-change/">Climate Change</a></li>
</ul>
</li>
<li id="menu-item-83" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-83"><a href="https://futureoflife.org/get-involved/">Get Involved</a></li>
<li id="menu-item-39" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-39"><a href="https://futureoflife.org/contact/">Contact</a></li>
</ul></nav> </div>
</div>
<div id='header_main' class='container_wrap container_wrap_logo'>
<div class='container av-logo-container'><div class='inner-container'>
<div id="headergraphics">
<div class="tagline"> Technology is giving life<br>the potential to flourish<br>like never before... </div>
<img src="/wp-content/themes/enfold/images/dead_tree.png?x40372" alt="dead tree image">
<div class="tagline"> ...or to self-destruct.<br>Let's make a difference! </div>
</div><strong class='logo'><a href='https://futureoflife.org/'><img height='100' width='300' src='https://futureoflife.org/wp-content/uploads/2015/10/FLI_logo-1.png?x40372' alt='Future of Life Institute' /></a></strong></div></div><div id='header_main_alternate' class='container_wrap'><div class='container'><nav class='main_menu' data-selectname='Select a page' role="navigation" itemscope="itemscope" itemtype="https://schema.org/SiteNavigationElement"><div class="avia-menu av-main-nav-wrap"><ul id="avia-menu" class="menu av-main-nav"><li id="menu-item-723" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-top-level menu-item-top-level-1"><a href="https://futureoflife.org/front-page-3/" itemprop="url"><span class="avia-bullet"></span><span class="avia-menu-text">News:</span><span class="avia-menu-fx"><span class="avia-arrow-wrap"><span class="avia-arrow"></span></span></span></a></li>
<li id="menu-item-280" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-top-level menu-item-top-level-2"><a href="https://futureoflife.org/ai-news/" itemprop="url"><span class="avia-bullet"></span><span class="avia-menu-text">AI</span><span class="avia-menu-fx"><span class="avia-arrow-wrap"><span class="avia-arrow"></span></span></span></a></li>
<li id="menu-item-537" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-top-level menu-item-top-level-3"><a href="https://futureoflife.org/biotechnology-news/" itemprop="url"><span class="avia-bullet"></span><span class="avia-menu-text">Biotech</span><span class="avia-menu-fx"><span class="avia-arrow-wrap"><span class="avia-arrow"></span></span></span></a></li>
<li id="menu-item-543" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-top-level menu-item-top-level-4"><a href="https://futureoflife.org/nuclear/" itemprop="url"><span class="avia-bullet"></span><span class="avia-menu-text">Nuclear</span><span class="avia-menu-fx"><span class="avia-arrow-wrap"><span class="avia-arrow"></span></span></span></a></li>
<li id="menu-item-542" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-top-level menu-item-top-level-5"><a href="https://futureoflife.org/environment/" itemprop="url"><span class="avia-bullet"></span><span class="avia-menu-text">Climate</span><span class="avia-menu-fx"><span class="avia-arrow-wrap"><span class="avia-arrow"></span></span></span></a></li>
<li id="menu-item-1409" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-top-level menu-item-top-level-6"><a href="https://futureoflife.org/news-from-our-partner-organizations/" itemprop="url"><span class="avia-bullet"></span><span class="avia-menu-text">Partner Orgs</span><span class="avia-menu-fx"><span class="avia-arrow-wrap"><span class="avia-arrow"></span></span></span></a></li>
<li id="menu-item-search" class="noMobile menu-item menu-item-search-dropdown">
<a href="?s=" rel="nofollow" data-avia-search-tooltip="

&lt;form action=&quot;https://futureoflife.org/&quot; id=&quot;searchform&quot; method=&quot;get&quot; class=&quot;&quot;&gt;
	&lt;div&gt;
		&lt;input type=&quot;submit&quot; value=&quot;&quot; id=&quot;searchsubmit&quot; class=&quot;button avia-font-entypo-fontello&quot; /&gt;
		&lt;input type=&quot;text&quot; id=&quot;s&quot; name=&quot;s&quot; value=&quot;&quot; placeholder='Search' /&gt;
			&lt;/div&gt;
&lt;/form&gt;" aria-hidden='true' data-av_icon='' data-av_iconfont='entypo-fontello'><span class="avia_hidden_link_text">Search</span></a>
</li></ul></div></nav></div> </div>

</div>
<div class='header_bg'></div>

</header>
<div id='main' data-scroll-offset='116'>
<div class='main_color container_wrap_first container_wrap fullsize'><div class='container'><main role="main" itemprop="mainContentOfPage" class='template-page content  av-content-full alpha units'><div class='post-entry post-entry-type-page post-entry-17345'><div class='entry-content-wrapper clearfix'><div class="flex_column av_one_full  flex_column_div av-zero-column-padding first  avia-builder-el-0  avia-builder-el-no-sibling  " style='border-radius:0px; '><div style='padding-bottom:10px;' class='av-special-heading av-special-heading-h1  blockquote modern-quote modern-centered  avia-builder-el-1  el_before_av_textblock  avia-builder-el-first  '><h1 class='av-special-heading-tag' itemprop="headline">2018 Project Grants Recommended for Funding</h1><div class='special-heading-border'><div class='special-heading-inner-border'></div></div></div>
<section class="av_textblock_section" itemscope="itemscope" itemtype="https://schema.org/CreativeWork"><div class='avia_textblock ' itemprop="text"><table class="grant_recipients" border="3" cellspacing="0" cellpadding="5">
<tbody>
<tr>
<td>Primary Investigator</td>
<td>Project Title</td>
<td>Amount Recommended</td>
<td>Email</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/2018-ai-grant-recipients#Dafoe">Allan Dafoe, Yale University</a></td>
<td><a href="http://futureoflife.org/2018-ai-grant-recipients#Dafoe">Governance of AI Programme</a></td>
<td>$276,000</td>
<td>allan.dafoe@yale.edu</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/2018-ai-grant-recipients#Ermon">Stefano Ermon, Stanford University</a></td>
<td><a href="http://futureoflife.org/2018-ai-grant-recipients#Ermon">Value Alignment and Multi-agent Inverse Reinforcement Learning</a></td>
<td>$100,000</td>
<td>ermon@cs.stanford.edu</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/2018-ai-grant-recipients#Evans">Owain Evans, Oxford University</a></td>
<td><a href="http://futureoflife.org/2018-ai-grant-recipients#Evans">Factored Cognition: Amplifying Human Cognition for Safely Scalable AGI</a></td>
<td>$225,000</td>
<td>owain.evans@philosophy.ox.ac.uk</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/2018-ai-grant-recipients#Han">The Anh Han, Teesside University</a></td>
<td><a href="http://futureoflife.org/2018-ai-grant-recipients#Han">Incentives for Safety Agreement Compliance in AI Race</a></td>
<td>$224,747</td>
<td>t.han@tees.ac.uk</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/2018-ai-grant-recipients#Hernandez">Jose Hernandez-Orallo, University of Cambridge</a></td>
<td><a href="http://futureoflife.org/2018-ai-grant-recipients#Hernandez">Paradigms of Artificial General Intelligence and Their Associated Risks</a></td>
<td>$220,000</td>
<td>jorallo@dsic.upv.es</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/2018-ai-grant-recipients#Hutter">Marcus Hutter, Australian National University</a></td>
<td><a href="http://futureoflife.org/2018-ai-grant-recipients#Hutter">The Control Problem for Universal AI: A Formal Investigation</a></td>
<td>$276,000</td>
<td>marcus.hutter@anu.edu.au</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/2018-ai-grant-recipients#Miller">James Miller, Smith College</a></td>
<td><a href="http://futureoflife.org/2018-ai-grant-recipients#Miller">Utility Functions: A Guide for Artificial General Intelligence Theorists</a></td>
<td>$78,289</td>
<td>jdmiller@smith.edu</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/2018-ai-grant-recipients#Sadigh">Dorsa Sadigh, Stanford University</a></td>
<td><a href="http://futureoflife.org/2018-ai-grant-recipients#Sadigh">Safe Learning and Verification of Human-AI Systems</a></td>
<td>$250,000</td>
<td>dorsa@cs.stanford.edu</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/2018-ai-grant-recipients#Stone">Peter Stone, University of Texas</a></td>
<td><a href="http://futureoflife.org/2018-ai-grant-recipients#Stone">Ad hoc Teamwork and Moral Feedback as a Framework for Safe Robot Behavior</a></td>
<td>$200,000</td>
<td>pstone@cs.utexas.edu</td>
</tr>
<tr>
<td><a href="http://futureoflife.org/2018-ai-grant-recipients#Tenenbaum">Josh Tenenbaum, MIT</a></td>
<td><a href="http://futureoflife.org/2018-ai-grant-recipients#Tenenbaum">Reverse Engineering Fair Cooperation</a></td>
<td>$150,000</td>
<td>jbt@mit.edu</td>
</tr>
</tbody>
</table>
<p></p>
<div>
<h2>Project Summaries</h2>
<h3><a class="adjust_anchor" name="Dafoe"></a><a href="http://www.allandafoe.com/">Allan Dafoe</a></h3>
<p><strong>Technical Abstract: </strong>​Artificial general intelligence (AGI) may be developed within this century. While this event could bring vast benefits, such as a dramatic acceleration of scientific and economic progress, it also poses important risks. A recent survey shows that the median AI researcher believes there is at least a one-in-twenty chance of a negative outcome as extreme as human extinction. Ensuring that AGI is developed safely and beneficially, and that the worst risks are avoided, will require institutions that do not yet exist. Nevertheless, the need to design and understand these institutions has so far inspired very little academic work. Our programme aims to address several questions that are foundational to the problem of governing advanced AI systems. We will pursue four workstreams toward this aim, concerning the state of Chinese AI research and policy thought, evolving relationships between governments and AI research firms, the prospects for verifying agreements on AI use and development, and strategically relevant properties of AI systems that may guide states&#8217; approaches to AI governance. Outputs of the programme will include academic publications, workshops, and consultations with leading actors in AI development and policy.</p>
<h3><a class="adjust_anchor" name="Ermon"></a><a href="https://cs.stanford.edu/~ermon/">Stefano Ermon</a></h3>
<p><strong>Technical Abstract: </strong>Reward specification, a key challenge in value alignment, is particularly difficult in environments with multiple agents, since the designer has to balance between individual gain and overall social utility. Instead of designing rewards by hand, we consider inverse reinforcement learning (IRL), an imitation learning technique where agents learn directly from human demonstrations. These techniques are well developed for the single agent case, and while they have limitations, they are often considered a key component for addressing the value alignment problem. Yet, multi-agent settings are relatively unexplored.</p>
<p>We propose to fill this gap and develop imitation learning and inverse reinforcement learning algorithms specifically designed for multi-agent settings. Our objectives are to: 1) develop techniques to imitate observed human behavior and interactions, 2) explicitly recover rewards that can explain complex strategic behaviors in multi-agent systems, enabling agents to reason about human behavior and safely co-exist, 3) develop interpretable techniques, and 4) deal with irrational agents to maximize safety. These methods will significantly improve our capabilities to understand and reason about the interactions among multiple agents in complex environments.</p>
<h3><a class="adjust_anchor" name="Evans"></a><a href="https://owainevans.github.io/">Owain Evans</a></h3>
<p><strong>Technical Abstract: </strong>Our goal is to understand how Machine Learning can be used for AGI in a way that is &#8216;safely scalable&#8217;, i.e. becomes increasingly aligned with human interests as the ML components improve. Existing approaches to AGI (including RL and IRL) are arguably not safely scalable: the agent can become un-aligned once its cognitive resources exceed those of the human overseer. Christiano&#8217;s Iterated Distillation and Amplification (IDA) is a promising alternative. In IDA, the human and agent are &#8216;amplified&#8217; into a resourceful (but slow) overseer by allowing the human to make calls to the previous iteration of the agent. By construction, this overseer is intended to always stay ahead of the agent being overseen.<br />
Could IDA produce highly capable aligned agents given sufficiently advanced ML components? While we cannot directly get empirical evidence today, we can study it indirectly by running amplification with humans as stand-ins for AI. This corresponds to the study of &#8216;factored cognition&#8217;, the question of whether sophisticated reasoning can be broken down into many small and mostly independent sub-tasks. We will explore schemes for factored cognition empirically and exploit automation via ML to tackle larger tasks.</p>
<h3><a class="adjust_anchor" name="Han"></a><a href="http://tees.ac.uk/schools/scm/staff_profile_details.cfm?staffprofileid=U0029688">The Anh Han</a></h3>
<p><strong>Technical Abstract: </strong>An AI race for technological advantage towards powerful AI systems could lead to serious negative consequences, especially when ethical and safety procedures are underestimated or even ignored. For all to enjoy the benefits provided by a safe, ethical and trustworthy AI, it is crucial to enact appropriate incentive strategies that ensure mutually beneficial, normative behaviour and safety-compliance from all parties involved. Using methods from Evolutionary Game Theory, this project will develop computational models (both analytic and simulated) that capture key factors of an AI race, revealing which strategic behaviours would likely emerge in different conditions and hypothetical scenarios of the race. Moreover, applying methods from incentives and agreement modelling, we will systematically analyse how different types of incentives (namely, positive vs. negative, peer vs. institutional, and their combinations) influence safety-compliance behaviours over time, and how such behaviours should be configured to ensure desired global outcomes, without undue restrictions that would slow down development. The project will thus provide foundations on which incentives will stimulate such outcomes, and how they need to be employed and deployed, within incentive boundaries suited to types of players, in order to achieve high level of compliance in a cooperative safety agreement and avoid AI disasters.</p>
<h3><a class="adjust_anchor" name="Hernandez"></a><a href="http://users.dsic.upv.es/~jorallo/">Jose Hernandez-Orallo</a></h3>
<p><strong>Technical Abstract: </strong>Many paradigms exist, and more will be created, for developing and understanding AI. Under these paradigms, the key benefits and risks materialise very differently. One dimension pervading all these paradigms is the notion of generality, which plays a central role, and provides the middle letter, in AGI, artificial general intelligence. This project explores the safety issues of present and future AGI paradigms from the perspective of measures of generality, as a complementary dimension to performance. We investigate the following research questions:<br />
1. Should we define generality in terms of tasks, goals or dominance? How does generality relate to capability, to computational resources, and ultimately to risks?<br />
2. What are the safe trade-offs between general systems with limited capability or less general systems with higher capability? How is this related to the efficiency and risks of automation?<br />
3. Can we replace the monolithic notion of performance explosion with breadth growth? How can this help develop safe pathways for more powerful AGI systems?<br />
These questions are analysed for paradigms such as reinforcement learning, inverse reinforcement learning, adversarial settings (Turing learning), oracles, cognition as a service, learning by demonstration, control or traces, teaching scenarios, curriculum and transfer learning, naturalised induction, cognitive architectures, brain-inspired AI, among others.</p>
<h3><a class="adjust_anchor" name="Hutter"></a><a href="http://web.mit.edu/cocosci/josh.html">Marcus Hutter</a></h3>
<p><strong>Technical Abstract: </strong>The agent framework, the expected utility principle, sequential decision theory, and the information-theoretic foundations of inductive reasoning and machine learning have already brought significant order into the previously heterogeneous scattered field of artificial intelligence (AI). Building on this, in the last decade I have developed the theory of Universal AI [Hut05]. It is the first and currently only mathematically rigorous top &#8216;down approach to formalize artificial general intelligence. This project will drive forward the theory of Universal AI to address what might be the 21st century&#8217;s most significant existential risk: solving the Control Problem, the unique principal-agent problem that arises with the creation of an artificial superintelligent agent [Bos14]. The goal is to extend the existing theory to enable formal investigations into the Control Problem for generally intelligent agents. Our focus is on the most essential properties that the theory of Universal AI lacks, namely a theory of agents embedded in the real world [EFDH16]: it does not model itself reliably, it is constraint to a single agent, it does not explore safely, and it is not well-understood how to specify goals that are aligned with human values [EH18a].</p>
<h3><a class="adjust_anchor" name="Miller"></a><a href="https://www.smith.edu/academics/faculty/james-miller">James Miller</a></h3>
<p><strong>Technical Abstract: </strong>Economists, having long labored to create mathematical tools that describe how hyper-rational people behave, might have devised an excellent means of modeling future computer superintelligences. This guide explains the uses, assumptions, and limitations of utility functions in the hope of becoming a valuable resource to artificial general intelligence (AGI) theorists. The guide will critique the AGI literature on instrumental convergence which theorizes that for many types of utility functions an AGI would have similar intermediate goals. The guide considers the orthogonality thesis, which holds that increasing an AGI&#8217;s intelligence does not shrink the set of utility functions it could have. This guide explores utility functions that might arise in an AGI but usually do not in economic research, such as those with instability, always increasing marginal utility, extremely high or low discount rates, those that can be self-modified, or those with preferences that violate one of the assumptions of the von Neumann-Morgenstern utility theorem. The guide considers the possibility that extraterrestrials have developed computer superintelligences that have converged on utility functions consistent with the Fermi paradox. Finally, the plausibility of an AGI getting its values from human utility functions, even given the challenge that humans have divergent preferences, is explored.</p>
<h3><a class="adjust_anchor" name="Sadigh"></a><a href="https://dorsa.fyi/">Dorsa Sadigh</a></h3>
<p><strong>Technical Abstract: </strong>Recent developments in artificial intelligence (AI) have enabled us to build AI agents and robots capable of performing complex tasks, including many that interact with humans. In these tasks, it is desirable for robots to build predictive and robust models of humans’ behaviors and preferences: a robot manipulator collaborating with a human needs to predict her future trajectories, or humans sitting in self-driving cars might have preferences for how cautiously the car should drive. In reality, humans have different preferences, which can be captured in the form of a mixture of reward functions. Learning this mixture can be challenging due to having different types of humans. It is also usually assumed that these humans are approximately optimizing the learned reward functions. However, in many safety-critical scenarios, humans follow behaviors that are not easily explainable by the learned reward functions due to lack of data or misrepresentation of the structure of the reward function. Our goal in this project is to actively learn a mixture of reward functions by eliciting comparisons from a mixed set of humans, and further analyze the generalizability and robustness of such models for safe and seamless interaction with AI agents.</p>
<h3><a class="adjust_anchor" name="Stone"></a><a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a></h3>
<p><strong>Technical Abstract: </strong>As technology develops, it is only a matter of time before agents will be capable of long term (general purpose) autonomy, i.e., will need to choose their actions by themselves for a long period of time. Thus, in many cases agents will not be able to be coordinated in advance with all other agents with which they may interact. Instead, agents will need to cooperate in order to accomplish unanticipated joint goals without pre-coordination. As a result, the &#8220;ad hoc teamwork&#8221; problem, in which teammates must work together to obtain a common goal without any prior agreement regarding how to do so, has emerged as a recent area of study in the AI literature. However, to date, no attention has been dedicated to the moral aspect of the agents&#8217; behavior. In this research, we introduce the M-TAMER framework (a novel variant of TAMER) used to teach agents the idea of human morality. Using a hybrid team (agents and people), if taking an action considered to be morally bad, the agents will receive negative feedback from the human teammate(s). Using M-TAMER, agents will be able to develop an &#8220;inner-conscience&#8221; which will enable them to act consistently with human morality.</p>
<h3><a class="adjust_anchor" name="Tenenbaum"></a><a href="http://web.mit.edu/cocosci/josh.html">Josh Tenenbaum</a></h3>
<p><strong>Technical Abstract: </strong>A hallmark of human cognition is the flexibility to plan with others across novel situations in the presence of uncertainty. We act together with partners of variable sophistication and knowledge and against adversaries who are themselves both heterogeneous and flexible. While a team of agents may be united by common goals, there are often multiple ways for the group to actually achieve those goals. In the absence of centralized planning or perception and constrained or costly communication, teams of agents must efficiently coordinate their plans with respect to the underlying differences across agents. Different agents may have different skills, competencies or access to knowledge. When environments and goals are changing, this coordination has elements of being ad-hoc. Miscoordination can lead to unsafe interactions and cause injury and property damage and so ad-hoc teamwork between humans and agents must be not only efficient but robust. We will both investigate human ad-hoc and dynamic collaboration and build formal computational models that reverse-engineer these capacities. These models are a key step towards building machines that can collaborate like people and with people.<br />
</p>
</div>
<div class="clearDiv"></div>
</div></section>
<div class='hr hr-default  avia-builder-el-3  el_after_av_textblock  avia-builder-el-last '><span class='hr-inner '><span class='hr-inner-style'></span></span></div></div>
</div></div></main>  </div></div> <div class='container_wrap footer_color' id='footer'>
<div class='container'>
<div class='flex_column av_one_fourth  first el_before_av_one_fourth'><section id="search-6" class="widget clearfix widget_search"><h3 class="widgettitle">Search for a topic:</h3>
<form action="https://futureoflife.org/" id="searchform" method="get" class="">
<div>
<input type="submit" value="" id="searchsubmit" class="button avia-font-entypo-fontello" />
<input type="text" id="s" name="s" value="" placeholder='Search' />
</div>
</form><span class="seperator extralight-border"></span></section></div><div class='flex_column av_one_fourth  el_after_av_one_fourth  el_before_av_one_fourth '><section id="categories-5" class="widget clearfix widget_categories"><h3 class="widgettitle">News and Information</h3><form action="https://futureoflife.org" method="get"><label class="screen-reader-text" for="cat">News and Information</label><select name='cat' id='cat' class='postform'>
<option value='-1'>Select Category</option>
<option class="level-0" value="4">AI</option>
<option class="level-0" value="12">AI background</option>
<option class="level-0" value="52">AI Research</option>
<option class="level-0" value="6">Biotech</option>
<option class="level-0" value="13">Biotech background</option>
<option class="level-0" value="54">Climate background</option>
<option class="level-0" value="29">Conferences and workshops</option>
<option class="level-0" value="7">Environment</option>
<option class="level-0" value="16">Environment background</option>
<option class="level-0" value="17">Events</option>
<option class="level-0" value="51">Existential Risk</option>
<option class="level-0" value="11">Featured</option>
<option class="level-0" value="10">FLI projects</option>
<option class="level-0" value="30">Grants Program</option>
<option class="level-0" value="57">Japan</option>
<option class="level-0" value="25">Newsletters</option>
<option class="level-0" value="28">Newsletters2</option>
<option class="level-0" value="5">Nuclear</option>
<option class="level-0" value="14">Nuclear background</option>
<option class="level-0" value="31">Open Letters</option>
<option class="level-0" value="15">Partner org background</option>
<option class="level-0" value="9">Partner Orgs</option>
<option class="level-0" value="27">Past Events</option>
<option class="level-0" value="55">principles</option>
<option class="level-0" value="56">principles interviews</option>
<option class="level-0" value="19">recent news</option>
<option class="level-0" value="33">Research Priorities</option>
<option class="level-0" value="1">Uncategorized</option>
</select>
</form>
<script type='text/javascript'>
/* <![CDATA[ */
(function() {
	var dropdown = document.getElementById( "cat" );
	function onCatChange() {
		if ( dropdown.options[ dropdown.selectedIndex ].value > 0 ) {
			dropdown.parentNode.submit();
		}
	}
	dropdown.onchange = onCatChange;
})();
/* ]]> */
</script>
<span class="seperator extralight-border"></span></section></div><div class='flex_column av_one_fourth  el_after_av_one_fourth  el_before_av_one_fourth '><section id="mc4wp_form_widget-2" class="widget clearfix widget_mc4wp_form_widget"><h3 class="widgettitle">Newsletter</h3><script>(function() {
	if (!window.mc4wp) {
		window.mc4wp = {
			listeners: [],
			forms    : {
				on: function (event, callback) {
					window.mc4wp.listeners.push({
						event   : event,
						callback: callback
					});
				}
			}
		}
	}
})();
</script><form id="mc4wp-form-1" class="mc4wp-form mc4wp-form-1530 mc4wp-form-basic" method="post" data-id="1530" data-name="Default sign-up form"><div class="mc4wp-form-fields"><p>
<label>First Name:</label>
<input type="text" name="FNAME" placeholder="Your first name">
</p><p>
<label>Email address: </label>
<input type="email" name="EMAIL" placeholder="Your email address" required />
</p>
<p>
<input type="submit" value="Sign up" />
</p></div><label style="display: none !important;">Leave this field empty if you're human: <input type="text" name="_mc4wp_honeypot" value="" tabindex="-1" autocomplete="off" /></label><input type="hidden" name="_mc4wp_timestamp" value="1543034332" /><input type="hidden" name="_mc4wp_form_id" value="1530" /><input type="hidden" name="_mc4wp_form_element_id" value="mc4wp-form-1" /><div class="mc4wp-response"></div></form><span class="seperator extralight-border"></span></section></div><div class='flex_column av_one_fourth  el_after_av_one_fourth  el_before_av_one_fourth '><section id="text-2" class="widget clearfix widget_text"> <div class="textwidget">Technology is giving life the potential to flourish like never before... Or to self destruct.
Let's make a difference!</div>
<span class="seperator extralight-border"></span></section></div>
</div>

</div>
<footer class='container_wrap socket_color' id='socket' role="contentinfo" itemscope="itemscope" itemtype="https://schema.org/WPFooter">
<div class='container'>
<span class='copyright'>© Copyright - FLI - Future of Life Institute&nbsp;&nbsp;<a href="/privacy-policy/">Please read our updated Privacy Policy</a></span>
<ul class='noLightbox social_bookmarks icon_count_2'><li class='social_bookmarks_twitter av-social-link-twitter social_icon_1'><a target='_blank' href='http://twitter.com/FLIxrisk' aria-hidden='true' data-av_icon='' data-av_iconfont='entypo-fontello' title='Twitter'><span class='avia_hidden_link_text'>Twitter</span></a></li><li class='social_bookmarks_facebook av-social-link-facebook social_icon_2'><a target='_blank' href='https://www.facebook.com/futureoflifeinstitute' aria-hidden='true' data-av_icon='' data-av_iconfont='entypo-fontello' title='Facebook'><span class='avia_hidden_link_text'>Facebook</span></a></li></ul>
</div>

</footer>

</div>
</div>
<script>
		( function ( body ) {
			'use strict';
			body.className = body.className.replace( /\btribe-no-js\b/, 'tribe-js' );
		} )( document.body );
		</script>
<script type='text/javascript'>
 /* <![CDATA[ */  
var avia_framework_globals = avia_framework_globals || {};
    avia_framework_globals.frameworkUrl = 'https://futureoflife.org/wp-content/themes/enfold-4/framework/';
    avia_framework_globals.installedAt = 'https://futureoflife.org/wp-content/themes/enfold-4/';
    avia_framework_globals.ajaxurl = 'https://futureoflife.org/wp-admin/admin-ajax.php';
/* ]]> */ 
</script>
<script> /* <![CDATA[ */var tribe_l10n_datatables = {"aria":{"sort_ascending":": activate to sort column ascending","sort_descending":": activate to sort column descending"},"length_menu":"Show _MENU_ entries","empty_table":"No data available in table","info":"Showing _START_ to _END_ of _TOTAL_ entries","info_empty":"Showing 0 to 0 of 0 entries","info_filtered":"(filtered from _MAX_ total entries)","zero_records":"No matching records found","search":"Search:","all_selected_text":"All items on this page were selected. ","select_all_link":"Select all pages","clear_selection":"Clear Selection.","pagination":{"all":"All","next":"Next","previous":"Previous"},"select":{"rows":{"0":"","_":": Selected %d rows","1":": Selected 1 row"}},"datepicker":{"dayNames":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"dayNamesShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"dayNamesMin":["S","M","T","W","T","F","S"],"monthNames":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthNamesShort":["January","February","March","April","May","June","July","August","September","October","November","December"],"nextText":"Next","prevText":"Prev","currentText":"Today","closeText":"Done"}};var tribe_system_info = {"sysinfo_optin_nonce":"63f79e445f","clipboard_btn_text":"Copy to clipboard","clipboard_copied_text":"System info copied","clipboard_fail_text":"Press \"Cmd + C\" to copy"};/* ]]> */ </script><script>(function() {function addEventListener(element,event,handler) {
	if(element.addEventListener) {
		element.addEventListener(event,handler, false);
	} else if(element.attachEvent){
		element.attachEvent('on'+event,handler);
	}
}function maybePrefixUrlField() {
	if(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {
		this.value = "http://" + this.value;
	}
}

var urlFields = document.querySelectorAll('.mc4wp-form input[type="url"]');
if( urlFields && urlFields.length > 0 ) {
	for( var j=0; j < urlFields.length; j++ ) {
		addEventListener(urlFields[j],'blur',maybePrefixUrlField);
	}
}/* test if browser supports date fields */
var testInput = document.createElement('input');
testInput.setAttribute('type', 'date');
if( testInput.type !== 'date') {

	/* add placeholder & pattern to all date fields */
	var dateFields = document.querySelectorAll('.mc4wp-form input[type="date"]');
	for(var i=0; i<dateFields.length; i++) {
		if(!dateFields[i].placeholder) {
			dateFields[i].placeholder = 'YYYY-MM-DD';
		}
		if(!dateFields[i].pattern) {
			dateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';
		}
	}
}

})();</script><script type='text/javascript' src='https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js?ver=2.2.10.1'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var mailoptin_globals = {"admin_url":"https:\/\/futureoflife.org\/wp-admin\/","public_js":"https:\/\/futureoflife.org\/wp-content\/plugins\/mailoptin\/..\/mailoptin\/vendor\/mailoptin\/core\/src\/assets\/js\/src","nonce":"f894c61f54","mailoptin_ajaxurl":"\/2018-ai-grant-recipients\/?mailoptin-ajax=%%endpoint%%","ajaxurl":"https:\/\/futureoflife.org\/wp-admin\/admin-ajax.php","split_test_start_label":"Start Test","split_test_pause_label":"Pause Test","is_customize_preview":"false","disable_impression_tracking":"false","chosen_search_placeholder":"Type to search","js_confirm_text":"Are you sure you want to do this?","js_clear_stat_text":"Are you sure you want to do this? Clicking OK will delete all your optin analytics records."};
/* ]]> */
</script>
<script type='text/javascript' src='https://futureoflife.org/wp-content/plugins/mailoptin/../mailoptin/vendor/mailoptin/core/src/assets/js/mailoptin.min.js?x40372'></script>
<script type='text/javascript' src='https://futureoflife.org/wp-content/themes/enfold-4/js/avia.js?x40372'></script>
<script type='text/javascript' src='https://futureoflife.org/wp-content/themes/enfold-4/js/shortcodes.js?x40372'></script>
<script type='text/javascript' src='https://futureoflife.org/wp-content/themes/enfold-4/js/aviapopup/jquery.magnific-popup.min.js?x40372'></script>
<script type='text/javascript' src='https://futureoflife.org/wp-includes/js/mediaelement/wp-mediaelement.min.js?x40372'></script>
<script type='text/javascript' src='https://futureoflife.org/wp-includes/js/comment-reply.min.js?x40372'></script>
<script type='text/javascript' src='https://futureoflife.org/wp-includes/js/wp-embed.min.js?x40372'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var mc4wp_forms_config = [];
/* ]]> */
</script>
<script type='text/javascript' src='https://futureoflife.org/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?x40372'></script>
<!--[if lte IE 9]>
<script type='text/javascript' src='https://futureoflife.org/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?x40372'></script>
<![endif]-->
<div id="cookie-notice" role="banner" class="cn-top bootstrap" style="color: #fff; background-color: #000;"><div class="cookie-notice-container"><span id="cn-notice-text">This website uses both functional and non-functional cookies. For the placement and reading of non-functional cookies, we require your prior consent. You can change the use of cookies later and adjust your preferences. </span><a href="#" id="cn-accept-cookie" data-cookie-set="accept" class="cn-set-cookie cn-button bootstrap button">I agree</a><a href="#" id="cn-refuse-cookie" data-cookie-set="refuse" class="cn-set-cookie cn-button bootstrap button">I do not agree</a><a href="https://futureoflife.org/privacy-policy/" target="_blank" id="cn-more-info" class="cn-more-info cn-button bootstrap button">Read more</a>
</div>
<div class="cookie-notice-revoke-container"><a href="#" class="cn-revoke-cookie cn-button bootstrap button">Revoke Cookies</a></div>
</div><a href='#top' title='Scroll to top' id='scroll-top-link' aria-hidden='true' data-av_icon='' data-av_iconfont='entypo-fontello'><span class="avia_hidden_link_text">Scroll to top</span></a>
<div id="fb-root"></div>
</body>
</html>
